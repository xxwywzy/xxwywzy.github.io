<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/resources/favicon/favicon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/resources/favicon/favicon.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/resources/favicon/favicon.png">
  <link rel="mask-icon" href="/resources/favicon/favicon.png" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css" integrity="sha256-CTSx/A06dm1B063156EVh15m6Y67pAjZZaQc89LLSrU=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"xxwywzy.github.io","root":"/","images":"/resources/img/","scheme":"Gemini","darkmode":true,"version":"8.18.2","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":true,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"livere","storage":true,"lazyload":false,"nav":null,"activeClass":"livere"},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="本篇博客为 deeplearning.ai 第一部分的学习笔记。">
<meta property="og:type" content="article">
<meta property="og:title" content="deeplearning.ai 第一部分：神经网络与深度学习">
<meta property="og:url" content="https://xxwywzy.github.io/2021/08/25/deep-1/">
<meta property="og:site_name" content="口仆">
<meta property="og:description" content="本篇博客为 deeplearning.ai 第一部分的学习笔记。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://media.zjubiomedit.com/2019-01-27-073818.png">
<meta property="og:image" content="http://media.zjubiomedit.com/2019-01-27-074913.png">
<meta property="og:image" content="http://media.zjubiomedit.com/2019-01-27-075314.png">
<meta property="og:image" content="http://media.zjubiomedit.com/2019-01-29-073740.png">
<meta property="og:image" content="http://media.zjubiomedit.com/2019-01-27-083324.png">
<meta property="og:image" content="http://media.zjubiomedit.com/2019-01-29-071355.png">
<meta property="og:image" content="http://media.zjubiomedit.com/2019-01-29-074845.png">
<meta property="og:image" content="http://media.zjubiomedit.com/2019-01-31-085056.png">
<meta property="og:image" content="http://media.zjubiomedit.com/2019-01-31-145550.png">
<meta property="og:image" content="http://media.zjubiomedit.com/2019-02-01-013035.png">
<meta property="og:image" content="http://media.zjubiomedit.com/2019-02-01-013542.png">
<meta property="og:image" content="http://media.zjubiomedit.com/2019-02-01-013922.png">
<meta property="og:image" content="http://media.zjubiomedit.com/2019-02-01-020940.png">
<meta property="og:image" content="http://media.zjubiomedit.com/2019-02-01-023434.png">
<meta property="og:image" content="http://media.zjubiomedit.com/2019-02-03-025828.png">
<meta property="og:image" content="http://media.zjubiomedit.com/2019-02-03-075840.png">
<meta property="article:published_time" content="2021-08-25T14:46:55.000Z">
<meta property="article:modified_time" content="2023-08-06T12:10:41.000Z">
<meta property="article:author" content="Zheyu Wang">
<meta property="article:tag" content="deeplearning.ai">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://media.zjubiomedit.com/2019-01-27-073818.png">


<link rel="canonical" href="https://xxwywzy.github.io/2021/08/25/deep-1/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://xxwywzy.github.io/2021/08/25/deep-1/","path":"2021/08/25/deep-1/","title":"deeplearning.ai 第一部分：神经网络与深度学习"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>deeplearning.ai 第一部分：神经网络与深度学习 | 口仆</title>
  











<link rel="stylesheet" href="/resources/fonts/longcang/longcang-regular.css" >
<link rel="stylesheet" href="/resources/fonts/lxgw/lxgwwenkailite-regular.css" >
  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">口仆</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">Long may the sunshine</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-culture"><a href="/culture/" rel="section"><i class="fa fa-heartbeat fa-fw"></i>MEME</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%BC%95%E8%A8%80"><span class="nav-number">1.</span> <span class="nav-text">深度学习引言</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">1.1.</span> <span class="nav-text">什么是神经网络？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0"><span class="nav-number">1.2.</span> <span class="nav-text">神经网络的监督学习</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BC%9A%E5%85%B4%E8%B5%B7"><span class="nav-number">1.3.</span> <span class="nav-text">为什么深度学习会兴起？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%80%9D%E7%BB%B4%E5%AF%BC%E5%9B%BE"><span class="nav-number">1.4.</span> <span class="nav-text">思维导图</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80"><span class="nav-number">2.</span> <span class="nav-text">神经网络的编程基础</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AC%A6%E5%8F%B7%E5%AE%9A%E4%B9%89"><span class="nav-number">2.1.</span> <span class="nav-text">符号定义</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92"><span class="nav-number">2.2.</span> <span class="nav-text">逻辑回归</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E7%9A%84%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0"><span class="nav-number">2.2.1.</span> <span class="nav-text">逻辑回归的代价函数</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="nav-number">2.3.</span> <span class="nav-text">梯度下降</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%B8%AD%E7%9A%84%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="nav-number">2.3.1.</span> <span class="nav-text">逻辑回归中的梯度下降</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%90%91%E9%87%8F%E5%8C%96"><span class="nav-number">2.4.</span> <span class="nav-text">向量化</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%90%91%E9%87%8F%E5%8C%96%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92"><span class="nav-number">2.4.1.</span> <span class="nav-text">向量化逻辑回归</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#pythonnumpy-%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0"><span class="nav-number">2.5.</span> <span class="nav-text">Python&#x2F;Numpy 使用笔记</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%9E%84%E5%BB%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">2.6.</span> <span class="nav-text">构建神经网络</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%80%9D%E7%BB%B4%E5%AF%BC%E5%9B%BE-1"><span class="nav-number">2.7.</span> <span class="nav-text">思维导图</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%B5%85%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">3.</span> <span class="nav-text">浅层神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A6%82%E8%BF%B0"><span class="nav-number">3.1.</span> <span class="nav-text">神经网络概述</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%8E%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E7%9A%84%E5%AF%B9%E6%AF%94"><span class="nav-number">3.1.1.</span> <span class="nav-text">与逻辑回归的对比</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%A1%A8%E7%A4%BA%E4%B8%8E%E8%AE%A1%E7%AE%97"><span class="nav-number">3.1.2.</span> <span class="nav-text">表示与计算</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0"><span class="nav-number">3.1.3.</span> <span class="nav-text">代码实现</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="nav-number">3.2.</span> <span class="nav-text">激活函数</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B8%B8%E8%A7%81%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="nav-number">3.2.1.</span> <span class="nav-text">常见激活函数</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#sigmoid"><span class="nav-number">3.2.1.1.</span> <span class="nav-text">sigmoid</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#tanh"><span class="nav-number">3.2.1.2.</span> <span class="nav-text">tanh</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#relu"><span class="nav-number">3.2.1.3.</span> <span class="nav-text">ReLU</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#leaky-relu"><span class="nav-number">3.2.1.4.</span> <span class="nav-text">leaky ReLU</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E7%9A%84%E9%9D%9E%E7%BA%BF%E6%80%A7"><span class="nav-number">3.2.2.</span> <span class="nav-text">激活函数的非线性</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E7%9A%84%E5%AF%BC%E6%95%B0"><span class="nav-number">3.2.3.</span> <span class="nav-text">激活函数的导数</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="nav-number">3.3.</span> <span class="nav-text">神经网络的梯度下降</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%9A%8F%E6%9C%BA%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="nav-number">3.4.</span> <span class="nav-text">随机初始化</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%80%9D%E7%BB%B4%E5%AF%BC%E5%9B%BE-2"><span class="nav-number">3.5.</span> <span class="nav-text">思维导图</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">4.</span> <span class="nav-text">深层神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A6%82%E8%BF%B0"><span class="nav-number">4.1.</span> <span class="nav-text">深层神经网络概述</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AC%A6%E5%8F%B7%E5%AE%9A%E4%B9%89-1"><span class="nav-number">4.1.1.</span> <span class="nav-text">符号定义</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B7%B1%E5%B1%82%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="nav-number">4.1.2.</span> <span class="nav-text">深层网络中的前向传播</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BB%B4%E6%95%B0%E7%9A%84%E7%A1%AE%E8%AE%A4"><span class="nav-number">4.1.3.</span> <span class="nav-text">维数的确认</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E8%BF%9B%E8%A1%8C%E6%B7%B1%E5%B1%82%E8%A1%A8%E7%A4%BA"><span class="nav-number">4.2.</span> <span class="nav-text">为什么要进行深层表示？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%A8%A1%E5%9D%97"><span class="nav-number">4.3.</span> <span class="nav-text">深层神经网络的模块</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD%E6%A8%A1%E5%9D%97"><span class="nav-number">4.3.1.</span> <span class="nav-text">前向传播模块</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E6%A8%A1%E5%9D%97"><span class="nav-number">4.3.2.</span> <span class="nav-text">反向传播模块</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%82%E6%95%B0%E4%B8%8E%E8%B6%85%E5%8F%82%E6%95%B0"><span class="nav-number">4.4.</span> <span class="nav-text">参数与超参数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E5%A4%A7%E8%84%91%E7%9A%84%E5%85%B3%E7%B3%BB"><span class="nav-number">4.5.</span> <span class="nav-text">深层神经网络与大脑的关系</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%80%9D%E7%BB%B4%E5%AF%BC%E5%9B%BE-3"><span class="nav-number">4.6.</span> <span class="nav-text">思维导图</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Zheyu Wang"
      src="/resources/favicon/avatar.png">
  <p class="site-author-name" itemprop="name">Zheyu Wang</p>
  <div class="site-description" itemprop="description">相信过程</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">85</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">28</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">58</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/xxwywzy" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;xxwywzy" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://weibo.com/xxwywzy" title="Weibo → https:&#x2F;&#x2F;weibo.com&#x2F;xxwywzy" rel="noopener me" target="_blank"><i class="fab fa-weibo fa-fw"></i>Weibo</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://twitter.com/xxwywzy" title="Twitter → https:&#x2F;&#x2F;twitter.com&#x2F;xxwywzy" rel="noopener me" target="_blank"><i class="fab fa-twitter fa-fw"></i>Twitter</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://instagram.com/xxwywzy" title="Instagram → https:&#x2F;&#x2F;instagram.com&#x2F;xxwywzy" rel="noopener me" target="_blank"><i class="fab fa-instagram fa-fw"></i>Instagram</a>
      </span>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://xxwywzy.github.io/2021/08/25/deep-1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/resources/favicon/avatar.png">
      <meta itemprop="name" content="Zheyu Wang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="口仆">
      <meta itemprop="description" content="相信过程">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="deeplearning.ai 第一部分：神经网络与深度学习 | 口仆">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          deeplearning.ai 第一部分：神经网络与深度学习
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2021-08-25 22:46:55" itemprop="dateCreated datePublished" datetime="2021-08-25T22:46:55+08:00">2021-08-25</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" itemprop="url" rel="index"><span itemprop="name">人工智能</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="name">课程笔记</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>4.5k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>15 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><div class="note info"><p>本篇博客为 deeplearning.ai 第一部分的学习笔记。</p>
</div>
<span id="more"></span>
<h1 id="深度学习引言">深度学习引言</h1>
<h2 id="什么是神经网络">什么是神经网络？</h2>
<p>神经网络就是由若干神经元组合而成的网络结构，其包含<strong>输入层</strong>、<strong>隐藏层</strong>和<strong>输出层</strong>。而含有<strong>多层隐藏层</strong>的神经网络即为<strong>深度神经网络</strong>。下图给出了一个深度神经网络的示意图。</p>
<p><img src="http://media.zjubiomedit.com/2019-01-27-073818.png" width=60%></p>
<h2 id="神经网络的监督学习">神经网络的监督学习</h2>
<p>在深度学习领域，目前为止几乎所有的经济价值都是由基于<strong>监督学习</strong>的神经网络所创造的。</p>
<p>对于不同的应用领域，我们需要不同类型的神经网络：</p>
<ul>
<li>对于房价预测和在线广告等应用，采用的是相对标准的神经网络</li>
<li>对于图像领域的应用，常常使用<strong>卷积神经网络（CNN</strong>）</li>
<li>对于序列数据，例如音频、语言等，常常使用<strong>循环神经网络（RNN）</strong>（注意不要与递归神经网络混淆）</li>
</ul>
<p><img src="http://media.zjubiomedit.com/2019-01-27-074913.png" width=80%></p>
<p>监督学习所处理的数据可以分为<strong>结构化</strong>与<strong>非结构化</strong>两种：</p>
<ul>
<li>结构化数据：数据以类似关系型数据库的表结构的形式展示</li>
<li>非结构化数据：音频、图像或文本等特征无法直接展示的数据</li>
</ul>
<p><img src="http://media.zjubiomedit.com/2019-01-27-075314.png" width=80%></p>
<h2 id="为什么深度学习会兴起">为什么深度学习会兴起？</h2>
<p>深度学习兴起的原因主要有三点： + 信息化社会带来的数据量的巨大提升 + 硬件更新带来更快的计算速度 + 神经网络算法的不断发展</p>
<h2 id="思维导图">思维导图</h2>
<p><img src="http://media.zjubiomedit.com/2019-01-29-073740.png" width=90%></p>
<h1 id="神经网络的编程基础">神经网络的编程基础</h1>
<p>经典的神经网络可以理解为逻辑回归的叠加。本节将介绍逻辑回归的基本原理及其程序实现。</p>
<h2 id="符号定义">符号定义</h2>
<ul>
<li><span class="math inline">\(x\)</span>：表示输入，维度为 <span class="math inline">\((n_x,1)\)</span></li>
<li><span class="math inline">\(y\)</span>：表示输出，取值为 <span class="math inline">\((0,1)\)</span></li>
<li><span class="math inline">\((x^{(i)},y^{(i)})\)</span>：表示第 <span class="math inline">\(i\)</span> 组数据</li>
<li><span class="math inline">\(m\)</span>：表示训练集的样本个数</li>
<li><span class="math inline">\(m_{test}\)</span>：表示测试集的样本个数</li>
<li><span class="math inline">\(X = [x^{(1)}, x^{(2)}, \ldots, x^{(m)}]\)</span>：表示所有训练数据集的输入值，维度为 <span class="math inline">\((n_x,m)\)</span></li>
<li><span class="math inline">\(Y = [y^{(1)}, y^{(2)}, \ldots, y^{(m)}]\)</span>：表示所有训练数据集的输出值，维度为 <span class="math inline">\((1,m)\)</span></li>
</ul>
<h2 id="逻辑回归">逻辑回归</h2>
<p>逻辑回归适用于二分类问题。其公式为： <span class="math display">\[
\hat{y}=P(y=1\mid x), where\;0\le\hat{y}\le1
\]</span></p>
<p>逻辑回归中具体的参数包括：</p>
<ul>
<li><p>输入特征向量：<span class="math inline">\(x \in \mathbb{R}^{n_x}\)</span></p></li>
<li><p>训练标签：<span class="math inline">\(y \in \{0,1\}\)</span></p></li>
<li><p>权重：<span class="math inline">\(w \in \mathbb{R}^{n_x}\)</span></p></li>
<li><p>输出：<span class="math inline">\(\hat{y} = \sigma(w^Tx+b)\)</span></p></li>
<li><p>sigmoid函数：<span class="math inline">\(s = \sigma(w^Tx+b) = \sigma(z) = \frac 1 {1+e^{-z}}\)</span></p></li>
</ul>
<p><img src="http://media.zjubiomedit.com/2019-01-27-083324.png" width=50%></p>
<h3 id="逻辑回归的代价函数">逻辑回归的代价函数</h3>
<p>为了训练参数 <span class="math inline">\(w\)</span> 和 <span class="math inline">\(b\)</span>，我们需要定义一个代价函数。而线性回归采用的最小二乘函数会导致局部最优（非凸优化），并不适用于逻辑回归。这里选择如下的损失函数（<strong>交叉熵函数</strong>），该损失函数本质上是由极大似然估计得出的。</p>
<p><span class="math display">\[
L(\hat{y^{(i)}},y^{(i)}) = -(y^{(i)}\log(\hat{y}^{(i)})+(1-y^{(i)})\log(1-\hat{y}^{(i)}))
\]</span></p>
<p>因此代价函数为： <span class="math display">\[
J(w,b) = \frac 1 m \sum_{i=1}^m L(\hat{y}^{(i)},y^{(i)}) = - \frac 1 m \sum_{i=1}^m[y^{(i)}\log(\hat{y}^{(i)})+(1-y^{(i)})\log(1-\hat{y}^{(i)})]
\]</span></p>
<p>需要注意在命名上，损失函数通常指计算单个训练样本的误差，而代价函数则是整个训练集损失函数的平均。</p>
<h2 id="梯度下降">梯度下降</h2>
<p>得到了代价函数后，我们需要一种方法来找出最小化代价函数的 <span class="math inline">\(w\)</span> 和 <span class="math inline">\(b\)</span>。容易证明代价函数是凸函数（能够找出全局最优），因此这里采用<strong>梯度下降</strong>法，不断沿着梯度方向更新参数，直至收敛</p>
<p><span class="math display">\[
w = w - \alpha * \frac {d(J(w,b) )} {dw} \\
b = b - \alpha * \frac {d(J(w,b))} {db}
\]</span></p>
<p>公式中的 <span class="math inline">\(\alpha\)</span> 称为学习速率，控制梯度更新的步幅。在逻辑回归中，一般初始化参数为 0。</p>
<h3 id="逻辑回归中的梯度下降">逻辑回归中的梯度下降</h3>
<p>在逻辑回归中，梯度下降涉及到复合求导，需要基于<strong>链式法则</strong>求解。课程中介绍了计算图的方法，能够更加直观地求解复合导数，而这其实可以看做一种简单的反向传播。</p>
<p>下面给出求解含有 m 个样本的逻辑回归的梯度下降的伪代码</p>
<p>变量名如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X1                  Feature</span><br><span class="line">X2                  Feature</span><br><span class="line">W1                  Weight of the first feature.</span><br><span class="line">W2                  Weight of the second feature.</span><br><span class="line">B                   Logistic Regression parameter.</span><br><span class="line">M                   Number of training examples</span><br><span class="line">Y(i)                Expected output of i</span><br></pre></td></tr></table></figure>
<p>基于复合求导得出的导数如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">d(a)  = d(l)/d(a) = -(y/a) + ((<span class="number">1</span>-y)/(<span class="number">1</span>-a))</span><br><span class="line">d(z)  = d(l)/d(z) = a - y</span><br><span class="line">d(W1) = X1 * d(z)</span><br><span class="line">d(W2) = X2 * d(z)</span><br><span class="line">d(B)  = d(z)</span><br></pre></td></tr></table></figure>
<p>伪代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">J = <span class="number">0</span>; dW1 = <span class="number">0</span>; dW2 =<span class="number">0</span>; dB = <span class="number">0</span>;                 <span class="comment"># Devs</span></span><br><span class="line">W1 = <span class="number">0</span>; W2 = <span class="number">0</span>; B=<span class="number">0</span>;                            <span class="comment"># Weights</span></span><br><span class="line"><span class="keyword">for</span> i = <span class="number">1</span> to m</span><br><span class="line">    <span class="comment"># Forward pass</span></span><br><span class="line">    z(i) = W1*X1(i) + W2*X2(i) + b</span><br><span class="line">    a(i) = Sigmoid(z(i))</span><br><span class="line">    J += (Y(i)*log(a(i)) + (<span class="number">1</span>-Y(i))*log(<span class="number">1</span>-a(i)))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Backward pass</span></span><br><span class="line">    dz(i) = a(i) - Y(i)</span><br><span class="line">    dW1 += dz(i) * X1(i)</span><br><span class="line">    dW2 += dz(i) * X2(i)</span><br><span class="line">    dB  += dz(i)</span><br><span class="line">J /= m</span><br><span class="line">dW1/= m</span><br><span class="line">dW2/= m</span><br><span class="line">dB/= m</span><br><span class="line">    </span><br><span class="line"><span class="comment"># Gradient descent</span></span><br><span class="line">W1 = W1 - alpha * dW1</span><br><span class="line">W2 = W2 - alpha * dW2</span><br><span class="line">B = B - alpha * dB</span><br></pre></td></tr></table></figure>
<p>上述伪代码实际上存在两组循环（迭代循环没有写出），会影响计算的效率，我们可以使用<strong>向量化</strong>来减少循环。</p>
<h2 id="向量化">向量化</h2>
<p>向量化可以避免循环，减少运算时间，Numpy 的函数库基本都是向量化版本。向量化可以在 CPU 或 GPU 上实现（通过 SIMD 操作），GPU 上速度会更快。</p>
<h3 id="向量化逻辑回归">向量化逻辑回归</h3>
<p>下面将仅使用一组循环来实现逻辑回归：</p>
<p>输入变量为：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X       Input Feature, X shape <span class="keyword">is</span> [Nx,m]</span><br><span class="line">Y       Expect Output, Y shape <span class="keyword">is</span> [Ny,m]</span><br><span class="line">W       Weight, W shape <span class="keyword">is</span> [Nx,<span class="number">1</span>]</span><br><span class="line">b       Parameter, b shape <span class="keyword">is</span> [<span class="number">1</span>,<span class="number">1</span>]</span><br></pre></td></tr></table></figure>
<p>向量化后的伪代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">W = np.zeros((Nx, <span class="number">1</span>))</span><br><span class="line">b = <span class="number">0</span></span><br><span class="line">dW = np.zeros((Nx, <span class="number">1</span>))</span><br><span class="line">db = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> <span class="built_in">iter</span> <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000</span>):</span><br><span class="line">    Z = np.dot(W.T, X) + b      <span class="comment"># Vectorization, then broadcasting, Z shape is (1, m)</span></span><br><span class="line">    A = <span class="number">1</span> / (<span class="number">1</span> + np.exp(-Z))    <span class="comment"># Vectorization, A shape is (1, m)</span></span><br><span class="line">  	dZ = A - Y                  <span class="comment"># Vectorization, dZ shape is (1, m)</span></span><br><span class="line">    dW = np.dot(X, dZ.T) / m    <span class="comment"># Vectorization, dW shape is (Nx, 1)</span></span><br><span class="line">    db = np.<span class="built_in">sum</span>(dZ) / m         <span class="comment"># Vectorization, db shape is (1, 1)</span></span><br><span class="line">    </span><br><span class="line">    W = W - alpha * dW</span><br><span class="line">    b = b - alpha * db</span><br></pre></td></tr></table></figure>
<h2 id="pythonnumpy-使用笔记">Python/Numpy 使用笔记</h2>
<p>下面介绍课程中提到的一些 python/numpy 的使用 tips。</p>
<p><strong>Tip 1：</strong> 在 Numpy 中，<code>obj.sum(axis = 0)</code> 按列求和，<code>obj.sum(axis = 1)</code> 按行求和，默认将所有元素求和。</p>
<p><strong>Tip 2：</strong> 在 Numpy 中，<code>obj.reshape(1, 4)</code> 将通过广播机制（broadcasting）重组矩阵 。reshape 操作的调用代价极低，可以放在任何位置。广播机制的原理参考下图：</p>
<p><img src="http://media.zjubiomedit.com/2019-01-29-071355.png" width=50%></p>
<p><strong>Tip 3：</strong> 关于矩阵 shape 的问题：如果不指定一个矩阵的 shape，将生成 "rank 1 array"，会导致其 shape 为 <code>(m, )</code>，无法进行转置。对于这种情况，需要进行 reshape。可以使用 <code>assert(a.shape == (5,1))</code> 来判断矩阵的 shape 是否正确</p>
<p><strong>Tip 4：</strong> 计算 Sigmoid 函数的导数：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">s = sigmoid(x)</span><br><span class="line">ds = s * (<span class="number">1</span> - s)</span><br></pre></td></tr></table></figure>
<p><strong>Tip 5：</strong> 如何将三维图片重组为一个向量：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">v = image.reshape(image.shape[<span class="number">0</span>]*image.shape[<span class="number">1</span>]*image.shape[<span class="number">2</span>],<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p><strong>Tip 6：</strong> 归一化输入矩阵后，梯度下降将收敛得更快。</p>
<h2 id="构建神经网络">构建神经网络</h2>
<p>构建一个神经网络一般包含以下步骤：</p>
<ol type="1">
<li>定义神经网络的结构</li>
<li>初始化模型参数</li>
<li>重复以下循环直至收敛：
<ul>
<li>计算当前的代价函数（前向传播）</li>
<li>计算当前的梯度（反向传播）</li>
<li>更新参数（梯度下降）</li>
</ul></li>
</ol>
<p>对于神经网络的训练，数据集的预处理与超参数（如学习速率）的调整十分重要。</p>
<h2 id="思维导图-1">思维导图</h2>
<p><img src="http://media.zjubiomedit.com/2019-01-29-074845.png" width=90%></p>
<h1 id="浅层神经网络">浅层神经网络</h1>
<h2 id="神经网络概述">神经网络概述</h2>
<h3 id="与逻辑回归的对比">与逻辑回归的对比</h3>
<p>逻辑回归的结构如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">X1  \  </span><br><span class="line">X2   ==&gt;  z = XW + B ==&gt; a = Sigmoid(z) ==&gt; l(a,Y)</span><br><span class="line">X3  /</span><br></pre></td></tr></table></figure>
<p>而一个单层神经网络的结构如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">X1  \  </span><br><span class="line">X2   =&gt;  z1 = XW1 + B1 =&gt; a1 = Sig(z1) =&gt; z2 = a1W2 + B2 =&gt; a2 = Sig(z2) =&gt; l(a2,Y)</span><br><span class="line">X3  /</span><br></pre></td></tr></table></figure>
<p>因此，我们可以将神经网络简单理解为逻辑回归的叠加。</p>
<h3 id="表示与计算">表示与计算</h3>
<p>本节将定义含有一层隐藏层的神经网络</p>
<ul>
<li><code>a0 = x</code> 表示输入层</li>
<li><code>a1</code> 表示隐藏层的激活值</li>
<li><code>a2</code> 表示输出层的激活值</li>
</ul>
<p>计算神经网络的层数时，我们一般不考虑输入层（即本节讨论的是两层神经网络）。下图给出了一个神经网络的前向传播计算公式：</p>
<p><img src="http://media.zjubiomedit.com/2019-01-31-085056.png" width=70%></p>
<p>在该网络中，隐藏层的神经元数量（<code>noOfHiddenNeurons</code>）为 4，输入的维数（<code>nx</code>）为 3。计算中涉及到的各个变量及其大小如下：</p>
<ul>
<li><code>W1</code> 是隐藏层的参数矩阵, 其形状为 <code>(noOfHiddenNeurons, nx)</code></li>
<li><code>b1</code> 是隐藏层的参数矩阵, 其形状为 <code>(noOfHiddenNeurons, 1)</code></li>
<li><code>z1</code> 是 <code>z1 = W1*X + b</code> 的计算结果，其形状为 <code>(noOfHiddenNeurons, 1)</code></li>
<li><code>a1</code> 是 <code>a1 = sigmoid(z1)</code> 的计算结果，其形状为 <code>(noOfHiddenNeurons, 1)</code></li>
<li><code>W2</code> 是输出层的参数矩阵，其形状为 <code>(1, noOfHiddenNeurons)</code></li>
<li><code>b2</code> 是输出层的参数矩阵，其形状为 <code>(1, 1)</code></li>
<li><code>z2</code> 是 <code>z2 = W2*a1 + b</code> 的计算结果，其形状为 <code>(1, 1)</code></li>
<li><code>a2</code> 是 <code>a2 = sigmoid(z2)</code> 的计算结果，其形状为 <code>(1, 1)</code></li>
</ul>
<h3 id="代码实现">代码实现</h3>
<p>两层神经网络前向传播的伪代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i = <span class="number">1</span> to m</span><br><span class="line">z[<span class="number">1</span>, i] = W1*x[i] + b1      <span class="comment"># shape of z[1, i] is (noOfHiddenNeurons,1)</span></span><br><span class="line">a[<span class="number">1</span>, i] = sigmoid(z[<span class="number">1</span>, i])  <span class="comment"># shape of a[1, i] is (noOfHiddenNeurons,1)</span></span><br><span class="line">z[<span class="number">2</span>, i] = W2*a[<span class="number">1</span>, i] + b2   <span class="comment"># shape of z[2, i] is (1,1)</span></span><br><span class="line">a[<span class="number">2</span>, i] = sigmoid(z[<span class="number">2</span>, i])  <span class="comment"># shape of a[2, i] is (1,1)</span></span><br></pre></td></tr></table></figure>
<p>如果对整个训练集进行向量化，得到新的 <code>X</code> 形状为 <code>(Nx, m)</code>，则新的伪代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Z1 = W1X + b1     <span class="comment"># shape of Z1 (noOfHiddenNeurons,m)</span></span><br><span class="line">A1 = sigmoid(Z1)  <span class="comment"># shape of A1 (noOfHiddenNeurons,m)</span></span><br><span class="line">Z2 = W2A1 + b2    <span class="comment"># shape of Z2 is (1,m)</span></span><br><span class="line">A2 = sigmoid(Z2)  <span class="comment"># shape of A2 is (1,m)</span></span><br></pre></td></tr></table></figure>
<p>其中样本数量 m 始终表示列的维数，<code>X</code> 可以写为 <code>A0</code>。</p>
<h2 id="激活函数">激活函数</h2>
<h3 id="常见激活函数">常见激活函数</h3>
<h4 id="sigmoid">sigmoid</h4>
<p><img src="http://media.zjubiomedit.com/2019-01-31-145550.png" width=30%></p>
<p>sigmoid 激活函数的取值范围是 [0,1] 。</p>
<p>注意 sigmoid 可能会导致梯度下降时更新速度较慢。其代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sigmoid = <span class="number">1</span> / (<span class="number">1</span> + np.exp(-z)) <span class="comment"># Where z is the input matrix</span></span><br></pre></td></tr></table></figure>
<h4 id="tanh">tanh</h4>
<p><img src="http://media.zjubiomedit.com/2019-02-01-013035.png" width=30%></p>
<p>tanh 激活函数的取值范围是 [-1,1]（sigmoid 函数的偏移版本）。</p>
<p>对隐藏层来说，tanh 比 sigmoid 的效果更好，因为其输出的平均值更接近0，这使得下一层数据更加靠近中心（便于梯度下降）。而 tanh 与 sigmoid 存在同样的缺点，即如果输入过大或过小，则斜率会趋近于0，导致梯度下降出现问题。</p>
<p>代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tanh = (np.exp(z) - np.exp(-z)) / (np.exp(z) + np.exp(-z)) <span class="comment"># Where z is the input matrix</span></span><br><span class="line">tanh = np.tanh(z)   <span class="comment"># Where z is the input matrix</span></span><br></pre></td></tr></table></figure>
<h4 id="relu">ReLU</h4>
<p><img src="http://media.zjubiomedit.com/2019-02-01-013542.png" width=30%></p>
<p>ReLU 函数可以解决梯度下降慢的问题（针对正数）。如果你的问题是二元分类（0或1），那么输出层使用 sigmoid，隐藏层使用 ReLU。</p>
<p>代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ReLU = np.maximum(<span class="number">0</span>,z) <span class="comment"># so if z is negative the slope is 0 and if z is positive the slope remains linear.</span></span><br></pre></td></tr></table></figure>
<h4 id="leaky-relu">leaky ReLU</h4>
<p><img src="http://media.zjubiomedit.com/2019-02-01-013922.png" width=30%></p>
<p>leaky RELU 与 ReLU 的区别在于当输入为负值时，斜率会较小（不为0）。它和 ReLU 同样有效，但大部分人使用 ReLU。</p>
<p>代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">leaky_ReLU = np.maximum(<span class="number">0.01</span>*z,z)  <span class="comment">#the 0.01 can be a parameter for your algorithm.</span></span><br></pre></td></tr></table></figure>
<p>目前激活函数的选择并没有普适性的准则，需要尝试各种激活函数（也可以参考前人的经验）</p>
<h3 id="激活函数的非线性">激活函数的非线性</h3>
<p>线性激活函数会输出线性的激活值，因此无论你有多少层隐藏层，激活都将是线性的（类似逻辑回归），这会使隐藏层会失去意义，无法处理复杂的问题。因此我们需要非线性的激活函数。</p>
<p>注意当输出是实数时，可能需要使用线性激活函数，但即便如此如果输出非负，那么使用 ReLU 函数更加合理。</p>
<h3 id="激活函数的导数">激活函数的导数</h3>
<p>sigmoid 函数：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">A = <span class="number">1</span> / (<span class="number">1</span> + np.exp(-z))</span><br><span class="line">dA = (<span class="number">1</span> / (<span class="number">1</span> + np.exp(-z))) * (<span class="number">1</span> - (<span class="number">1</span> / (<span class="number">1</span> + np.exp(-z))))</span><br><span class="line">dA = A * (<span class="number">1</span> - A)</span><br></pre></td></tr></table></figure>
<p>tanh 函数：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">A = (np.exp(z) - np.exp(-z)) / (np.exp(z) + np.exp(-z))</span><br><span class="line">dA = <span class="number">1</span> - np.tanh(z)^<span class="number">2</span> = <span class="number">1</span> - A^<span class="number">2</span></span><br></pre></td></tr></table></figure>
<p>ReLU 函数：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">A = np.maximum(<span class="number">0</span>,z)</span><br><span class="line">dA = &#123; <span class="number">0</span>  <span class="keyword">if</span> z &lt; <span class="number">0</span></span><br><span class="line">       <span class="number">1</span>  <span class="keyword">if</span> z &gt;= <span class="number">0</span>  &#125;</span><br></pre></td></tr></table></figure>
<p>leaky ReLU 函数：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">A = np.maximum(<span class="number">0.01</span>*z,z)</span><br><span class="line">dA = &#123; <span class="number">0</span>  <span class="keyword">if</span> z &lt; <span class="number">0</span></span><br><span class="line">       <span class="number">1</span>  <span class="keyword">if</span> z &gt;= <span class="number">0</span>  &#125;</span><br></pre></td></tr></table></figure>
<h2 id="神经网络的梯度下降">神经网络的梯度下降</h2>
<p>反向传播的公式与伪代码如下：</p>
<p><img src="http://media.zjubiomedit.com/2019-02-01-020940.png" width=70%></p>
<h2 id="随机初始化">随机初始化</h2>
<p>在逻辑回归中随机初始化权重并不重要，而在神经网络中我们需要进行随机初始化。</p>
<p>如果在神经网络中将所有权重初始化为0，那么神经网络将不能正常工作：所有隐藏层会完全同步变化（计算同一个函数），每次梯度下降迭代所有隐藏层会进行相同的更新。注意 bias 初始化为0是可以的</p>
<p>为了解决这个问题我们将 W 初始化为一个小的随机数：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">W1 = np.random.randn((<span class="number">2</span>,<span class="number">2</span>)) * <span class="number">0.01</span>    <span class="comment"># 0.01 to make it small enough</span></span><br><span class="line">b1 = np.zeros((<span class="number">2</span>,<span class="number">1</span>))                  <span class="comment"># its ok to have b as zero</span></span><br></pre></td></tr></table></figure>
<p>对于 sigmoid 或 tanh 来说，我们需要随机数较小，因为较大的值会导致在训练初期线性激活输出过大，从而使激活函数趋向饱和，导致学习速度下降。而如果没有使用 sigmoid 或 tanh 作为激活函数，就不会有很大影响。</p>
<p>常数 0.01 对单层隐藏层来说是合适的，但对于更深的神经网络来说，这个参数会发生改变来保证线性计算得出的值不会过大。</p>
<h2 id="思维导图-2">思维导图</h2>
<p><img src="http://media.zjubiomedit.com/2019-02-01-023434.png" width=80%></p>
<h1 id="深层神经网络">深层神经网络</h1>
<h2 id="深层神经网络概述">深层神经网络概述</h2>
<p>深层神经网络是指隐藏层超过<strong>两层</strong>的神经网络：</p>
<p><img src="http://media.zjubiomedit.com/2019-02-03-025828.png" width=60%></p>
<h3 id="符号定义-1">符号定义</h3>
<ul>
<li>我们使用 <code>L</code> 来定义神经网络的层数（不包含输入层）</li>
<li><code>n</code> 表示每一层的神经元数量集合
<ul>
<li><code>n[0]</code> 表示输入层的维数</li>
<li><code>n[L]</code> 表示输出层的维数</li>
</ul></li>
<li><code>g</code> 表示每一层的激活函数</li>
<li><code>z</code> 表示每一层的线性输出
<ul>
<li><code>Z</code> 表示向量化后的线性输出</li>
</ul></li>
<li><code>w</code> 和 <code>b</code> 表示每一层线性输出的对应参数
<ul>
<li><code>W</code> 和 <code>B</code> 表示向量化后的参数</li>
</ul></li>
<li><code>a</code> 表示每一层的激活输出
<ul>
<li><code>a[0]</code> 表示输出，<code>a[L]</code> 表示输出</li>
<li><code>A</code> 表示向量化后的激活输出</li>
</ul></li>
</ul>
<h3 id="深层网络中的前向传播">深层网络中的前向传播</h3>
<p>对于单个输入，前向传播的伪代码如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">z[l] = W[l]a[l-1] + b[l]</span><br><span class="line">a[l] = g[l](z[l])</span><br></pre></td></tr></table></figure>
<p>对于 <code>m</code> 个输入（向量化），前向传播的伪代码如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Z[l] = W[l]A[l-1] + B[l]</span><br><span class="line">A[l] = g[l](Z[l])</span><br></pre></td></tr></table></figure>
<p>我们无法对整个前向传播使用向量化，需要使用 for 循环（即每一层要分开计算）。</p>
<h3 id="维数的确认">维数的确认</h3>
<p>我们需要确保各个向量的维数能够匹配，这里用 <code>l</code> 表示当前是第几层。各向量的具体维数如下：</p>
<ul>
<li><code>w[l]</code> 和 <code>dw[l]</code> 的维数：<code>(n[l], n[l-1])</code>
<ul>
<li><code>W[l]</code> 和 <code>dW[l]</code> 的维数： <code>(n[l], n[l-1])</code></li>
</ul></li>
<li><code>b[l]</code> 和 <code>db[l]</code> 的维数：<code>(n[l], 1)</code>
<ul>
<li><code>B[l]</code> 和 <code>dB[l]</code> 的维数： <code>(n[l], m)</code></li>
</ul></li>
<li><code>z[l]</code> 和 <code>a[l]</code> 的维数：<code>(n[l], 1)</code>
<ul>
<li><code>Z[l]</code> 和 <code>A[l]</code> 的维数：<code>(n[l], m)</code></li>
<li><code>dZ[l]</code> 和 <code>dA[l]</code> 的维数：<code>(n[l], m)</code></li>
</ul></li>
</ul>
<h2 id="为什么要进行深层表示">为什么要进行深层表示？</h2>
<p>我们可以从两个角度解释为什么使用多个隐藏层：</p>
<ul>
<li>多个隐藏层可以将问题从简单到复杂进行拆分，先考虑简单的特征，再逐步变得复杂，最终实现预期的效果；</li>
<li>电路理论表明越少的层数需要的单元数呈指数级上升，对神经网络来说也是如此。对于一个复杂任务来说，层数越少每一层所要包含的神经元数量会爆炸式增长。</li>
</ul>
<h2 id="深层神经网络的模块">深层神经网络的模块</h2>
<p>深层神经网络一般包含<strong>前向传播</strong>与<strong>反向传播</strong>两个模块：前向传播模块得到代价函数，后向传播模块计算各层参数的梯度，最后通过梯度下降来更新参数，进行学习。</p>
<p>在实际实现中，我们需要通过缓存将前向传播中的某些参数传递到反向传播中，帮助进行梯度的计算。</p>
<h3 id="前向传播模块">前向传播模块</h3>
<p>向量化后的伪代码如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input  A[l-1]</span><br><span class="line">Z[l] = W[l]A[l-1] + B[l]</span><br><span class="line">A[l] = g[l](Z[l])</span><br><span class="line">Output A[l], cache(Z[l], W[l], B[l])</span><br></pre></td></tr></table></figure>
<h3 id="反向传播模块">反向传播模块</h3>
<p>向量化后的伪代码如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Input dA[l], Caches</span><br><span class="line">dZ[l] = dA[l] * g&#x27;[l](Z[l])</span><br><span class="line">dW[l] = (1/m) * np.dot(dZ[l], A[l-1].T)</span><br><span class="line">dB[l] = (1/m) * np.sum(dZ[l], axis=1, keepdims=True)</span><br><span class="line">dA[l-1] = np.dot(W[l].T, dZ[l])</span><br><span class="line">Output dA[l-1], dW[l], dB[l]</span><br></pre></td></tr></table></figure>
<p>最后一层 <code>dA</code> 的求解基于代价函数得出，注意计算时应去除 <code>1/m</code> 这一项，防止重复计算。</p>
<h2 id="参数与超参数">参数与超参数</h2>
<p>在神经网络中，参数主要指 <code>w</code> 和 <code>b</code>。而超参数指影响参数选择的参数，例如：</p>
<ul>
<li>学习速率</li>
<li>迭代次数</li>
<li>隐藏层层数</li>
<li>隐藏层单元数</li>
<li>激励函数的选择</li>
</ul>
<p>深度学习是一个经验主义的过程，随着外界条件的不断变化，需要进行多次的实验来确定最佳的超参数与参数。</p>
<h2 id="深层神经网络与大脑的关系">深层神经网络与大脑的关系</h2>
<p>神经网络的单个逻辑单元与实际的神经元在<strong>结构</strong>上有一些相似，但大脑的工作原理目前还是未知的，所以无法进行进一步比较。</p>
<h2 id="思维导图-3">思维导图</h2>
<p><img src="http://media.zjubiomedit.com/2019-02-03-075840.png" width=90%></p>

    </div>

    
    
    

    <footer class="post-footer">




<div class="license">
  <div class="license-title">deeplearning.ai 第一部分：神经网络与深度学习</div>
  <div class="license-link">
    <a href="https://xxwywzy.github.io/2021/08/25/deep-1/">https://xxwywzy.github.io/2021/08/25/deep-1/</a>
  </div>
  <div class="license-meta">
    <div class="license-meta-item">
      <div class="license-meta-title">本文作者</div>
      <div class="license-meta-text">
          Zheyu Wang
      </div>
    </div>
      <div class="license-meta-item">
        <div class="license-meta-title">发布于</div>
        <div class="license-meta-text">
          2021-08-25
        </div>
      </div>
      <div class="license-meta-item">
        <div class="license-meta-title">更新于</div>
        <div class="license-meta-text">
          2023-08-06
        </div>
      </div>
    <div class="license-meta-item">
      <div class="license-meta-title">许可协议</div>
      <div class="license-meta-text">
          <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" rel="noopener" target="_blank">CC BY-NC-SA 4.0</a>
      </div>
    </div>
  </div>
  <div class="license-statement">
      转载或引用本文时，请遵守上述许可协议，注明出处、不得用于商业用途！
  </div>
</div>
          <div class="post-tags">
              <a href="/tags/deeplearning-ai/" rel="tag"># deeplearning.ai</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2021/08/24/dda-2/" rel="prev" title="《数据密集型应用系统设计》读书笔记（二）">
                  <i class="fa fa-angle-left"></i> 《数据密集型应用系统设计》读书笔记（二）
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2021/12/20/dda-3/" rel="next" title="《数据密集型应用系统设计》读书笔记（三）">
                  《数据密集型应用系统设计》读书笔记（三） <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments" id="lv-container" data-id="city" data-uid="MTAyMC81ODgyNi8zNTI4OA=="></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2023</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Zheyu Wang</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
      <span>站点总字数：</span>
    <span title="站点总字数">332k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span>站点阅读时长 &asymp;</span>
    <span title="站点阅读时长">18:26</span>
  </span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  






  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<script src="/js/third-party/comments/livere.js"></script>



  <style>
    #taboola-livere { display: none;}
  </style>



<script type="text/javascript">
var linkLists = document.querySelectorAll(".link-list");

linkLists.forEach(function(linkList) {
  var listPath = linkList.getAttribute('json-src');
  var iconPath = linkList.getAttribute('icon-src');
  
  var xhr = new XMLHttpRequest();
  xhr.open('GET', listPath, true);
  xhr.onreadystatechange = function() {
    if (xhr.readyState === 4 && xhr.status === 200) {
      var data = JSON.parse(xhr.responseText);
      
      var li = "";
      linkList.innerHTML = '';

      for (var infoIndex = 0; infoIndex < data.length; infoIndex++) {
        var info = data[infoIndex];
        var labelWarn = info['warn'] ? '<span class="label warn">' + info['warn'] + '</span>' : '';
        var labelInfo = info['info'] ? '<span class="label info">' + info['info'] + '</span>' : '';

        li += '<div class="link-list-container">';
        li += '<img class="link-list-image" src="' + iconPath + info['logo'] + '">';
        li += '<p>' + info['title'] + labelInfo + labelWarn + '</p>';
        li += '<p>' + info['intro'] + '</p>';
        li += '<a href="' + info['url'] + '" rel="noopener" target="_blank" data-pjax-state=""></a>';
        li += '</div>';
      }
      
      linkList.innerHTML = li;
    }
  };
  xhr.send();
});
</script>


<script type="text/javascript">
var cultureList = document.querySelectorAll(".culture-list");
if (cultureList.length !== 0) {
  var j = -1;
  for (var i = 0; i < cultureList.length; i++) {
    const listPath = cultureList[i].getAttribute('json-src');
    const coverPath = cultureList[i].getAttribute('cover-src');
    
    var xhr = new XMLHttpRequest();
    xhr.open('GET', listPath, true);
    xhr.onreadystatechange = function () {
      if (xhr.readyState === 4 && xhr.status === 200) {
        j++;
        var data = JSON.parse(xhr.responseText);
        var li = "";
        
        cultureList[j].innerHTML = '';

        for (var infoIndex = 0; infoIndex < data.length; infoIndex++) {
          var info = data[infoIndex];
          
          var title = info['title'];
          if (info['link']) {
            title = '<a href="' + info['link'] + '">' + info['title'] + '</a>';
          }

          var author = info['author'] ? '<span class="author">' + info['author'] + '</span>' : '';

          var intro = info['intro'] ? info['intro'] : '';

          var star = '';
          if (info['score'] == null) {
            star = '';
          } else {
            var colorStar = '';
            var greyStar = '';
            var int = Math.floor(info['score']); //整数部分
            var fract = 0;
            if (info['score'] % 1 !== 0) {
              fract = 1;
            }
            for (var m = 0; m < int; m++) {
              colorStar += '★';
            }
            if (fract !== 0) {
              colorStar += '☆';
            }
            for (var m = 0; m < (5 - fract - int); m++) {
              greyStar += '☆';
            }
            if (info['score'] !== 5) {
              star = '<span class="star-score">' + colorStar + '<span class="grey-star">' + greyStar + '</span></span>';
            } else {
              star = '<span class="star-score">' + colorStar + '</span>';
            }
          }

          li += '<div class="media">';
          li += '<div class="media-cover" style="background-image:url(' + coverPath + info['cover'] + ')"></div>';
          li += '<div class="media-meta">';
          li += '<div class="media-meta-item title">' + title + '</div>';
          li += '<div class="media-meta-item">' + author + star + '</div>';
          li += '<div class="media-meta-item intro">' + intro + '</div>';
          li += '</div></div>';
        }
        
        cultureList[j].innerHTML = li;
      }
    };
    xhr.send();
  }
}
</script>




<script src="/resources/minigrid.min.js"></script>
<script type="text/javascript">
var album = document.querySelector(".album");
if (album) {
  // 相册列表 JSON 数据
  var imgDataPath = album.getAttribute('json-src');
  // 照片存储路径
  var imgPath = album.getAttribute('photo-src');
  // 最多显示数量
  var imgMaxNum = 50;
  // 获取窗口大小以决定图片宽度
  var windowWidth = window.innerWidth || document.documentElement.clientWidth || document.body.clientWidth;
  var imageWidth;

  if (windowWidth < 768) {
    imageWidth = 145; // 移动端图片宽度
  } else {
    imageWidth = 235;
  }

  // 腾讯云自定义样式 (数据万象外网流量需要付费)
  //var imgStyle = '!' + imageWidth + 'x';
  //var imgStyle = '!300x';

  // 生成相册
  var linkDataPath = imgDataPath;
  var photo = {
    page: 1,
    offset: imgMaxNum,
    init: function () {
      var that = this;
      var xhr = new XMLHttpRequest();
      xhr.open("GET", linkDataPath, true);
      xhr.onreadystatechange = function () {
        if (xhr.readyState === 4 && xhr.status === 200) {
          var data = JSON.parse(xhr.responseText);
          that.render(that.page, data);
        }
      };
      xhr.send();
    },
    render: function (page, data) {
      var begin = (page - 1) * this.offset;
      var end = page * this.offset;
      if (begin >= data.length) return;
      var imgNameWithPattern, imgName, imageSize, imageX, imageY, li = "";
      for (var i = begin; i < end && i < data.length; i++) {
        imgNameWithPattern = data[i].split(' ')[1];
        imgName = imgNameWithPattern.split('.')[0];
        imageSize = data[i].split(' ')[0];
        imageX = imageSize.split('.')[0];
        imageY = imageSize.split('.')[1];
        li += '<div class="card" style="width:' + imageWidth + 'px" >';
        li += '<div class="album-photo" style="height:'+ imageWidth * imageY / imageX + 'px">';
        li += '<a class="fancybox fancybox.image" href="' + imgPath + imgNameWithPattern + '" itemscope="" itemtype="http://schema.org/ImageObject" itemprop="url" data-fancybox="group" rel="group" data-caption="' + imgName + '" title="' +  imgName + '">';
        li += '<img data-src="' + imgPath + imgNameWithPattern + '" src="' + imgPath + imgNameWithPattern + '" alt="' +  imgName + '" data-loaded="true">';
        li += '</a>';
        li += '</div>';
        li += '</div>';
      }
      album.insertAdjacentHTML('beforeend', li);
      this.minigrid();
    },
    minigrid: function () {
      var grid = new Minigrid({
        container: '.album',
        item: '.card',
        gutter: 12
      });
      grid.mount();
      window.addEventListener('resize', function () {
        grid.mount();
      });
    }
  };
  photo.init();
}
</script>
</body>
</html>
