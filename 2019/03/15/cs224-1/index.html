<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/resources/favicon/favicon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/resources/favicon/favicon.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/resources/favicon/favicon.png">
  <link rel="mask-icon" href="/resources/favicon/favicon.png" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css" integrity="sha256-CTSx/A06dm1B063156EVh15m6Y67pAjZZaQc89LLSrU=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"xxwywzy.github.io","root":"/","images":"/resources/img/","scheme":"Gemini","darkmode":true,"version":"8.18.2","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":true,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"livere","storage":true,"lazyload":false,"nav":null,"activeClass":"livere"},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="本篇博客为 CS224N 学习笔记第一部分，主题是：词向量（简介，SVD 和 Word2Vec）。">
<meta property="og:type" content="article">
<meta property="og:title" content="CS224N 学习笔记之一：词向量 1">
<meta property="og:url" content="https://xxwywzy.github.io/2019/03/15/cs224-1/">
<meta property="og:site_name" content="口仆">
<meta property="og:description" content="本篇博客为 CS224N 学习笔记第一部分，主题是：词向量（简介，SVD 和 Word2Vec）。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://media.zjubiomedit.com/2019-03-05-033032.png">
<meta property="og:image" content="http://media.zjubiomedit.com/2019-03-05-055909.png">
<meta property="og:image" content="http://media.zjubiomedit.com/2019-03-05-060148.png">
<meta property="og:image" content="http://media.zjubiomedit.com/2019-03-12-070940.png">
<meta property="og:image" content="http://media.zjubiomedit.com/2019-03-14-064627.png">
<meta property="og:image" content="http://media.zjubiomedit.com/2019-03-15-032628.jpg">
<meta property="og:image" content="http://media.zjubiomedit.com/2019-03-15-085129.png">
<meta property="article:published_time" content="2019-03-15T07:57:07.000Z">
<meta property="article:modified_time" content="2023-08-05T07:13:17.000Z">
<meta property="article:author" content="Zheyu Wang">
<meta property="article:tag" content="CS224N">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://media.zjubiomedit.com/2019-03-05-033032.png">


<link rel="canonical" href="https://xxwywzy.github.io/2019/03/15/cs224-1/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://xxwywzy.github.io/2019/03/15/cs224-1/","path":"2019/03/15/cs224-1/","title":"CS224N 学习笔记之一：词向量 1"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>CS224N 学习笔记之一：词向量 1 | 口仆</title>
  











<link rel="stylesheet" href="/resources/fonts/longcang/longcang-regular.css" >
<link rel="stylesheet" href="/resources/fonts/lxgw/lxgwwenkailite-regular.css" >
  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">口仆</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">Long may the sunshine</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-culture"><a href="/culture/" rel="section"><i class="fa fa-heartbeat fa-fw"></i>MEME</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AE%80%E4%BB%8B"><span class="nav-number">1.</span> <span class="nav-text">自然语言处理简介</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%9A%84%E7%89%B9%E6%AE%8A%E6%80%A7"><span class="nav-number">1.1.</span> <span class="nav-text">自然语言处理的特殊性</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BB%BB%E5%8A%A1%E7%B1%BB%E5%9E%8B"><span class="nav-number">1.2.</span> <span class="nav-text">任务类型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%A6%82%E4%BD%95%E8%A1%A8%E7%A4%BA%E8%AF%8D%E8%AF%AD"><span class="nav-number">1.3.</span> <span class="nav-text">如何表示词语</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%BC%A0%E7%BB%9F%E8%AF%8D%E5%90%91%E9%87%8F"><span class="nav-number">2.</span> <span class="nav-text">传统词向量</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E-svd-%E7%9A%84%E6%96%B9%E6%B3%95"><span class="nav-number">3.</span> <span class="nav-text">基于 SVD 的方法</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AF%8D%E8%AF%AD-%E6%96%87%E6%A1%A3%E7%9F%A9%E9%98%B5"><span class="nav-number">3.1.</span> <span class="nav-text">词语-文档矩阵</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E7%AA%97%E5%8F%A3%E7%9A%84%E5%85%B1%E7%8E%B0%E7%9F%A9%E9%98%B5"><span class="nav-number">3.2.</span> <span class="nav-text">基于窗口的共现矩阵</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%B0%86-svd-%E5%BA%94%E7%94%A8%E5%88%B0%E5%85%B1%E7%8E%B0%E7%9F%A9%E9%98%B5"><span class="nav-number">3.3.</span> <span class="nav-text">将 SVD 应用到共现矩阵</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AD%98%E5%9C%A8%E7%9A%84%E9%97%AE%E9%A2%98"><span class="nav-number">3.4.</span> <span class="nav-text">存在的问题</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E8%BF%AD%E4%BB%A3%E7%9A%84%E6%96%B9%E6%B3%95word2vec"><span class="nav-number">4.</span> <span class="nav-text">基于迭代的方法：Word2vec</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B"><span class="nav-number">4.1.</span> <span class="nav-text">语言模型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#cbow"><span class="nav-number">4.2.</span> <span class="nav-text">CBOW</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AC%A6%E5%8F%B7%E5%AE%9A%E4%B9%89"><span class="nav-number">4.2.1.</span> <span class="nav-text">符号定义</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AE%A1%E7%AE%97%E6%AD%A5%E9%AA%A4"><span class="nav-number">4.2.2.</span> <span class="nav-text">计算步骤</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95"><span class="nav-number">4.2.3.</span> <span class="nav-text">学习方法</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#skip-gram"><span class="nav-number">4.3.</span> <span class="nav-text">Skip-Gram</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AC%A6%E5%8F%B7%E5%AE%9A%E4%B9%89-1"><span class="nav-number">4.3.1.</span> <span class="nav-text">符号定义</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AE%A1%E7%AE%97%E6%AD%A5%E9%AA%A4-1"><span class="nav-number">4.3.2.</span> <span class="nav-text">计算步骤</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-1"><span class="nav-number">4.3.3.</span> <span class="nav-text">学习方法</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#negative-sampling"><span class="nav-number">4.4.</span> <span class="nav-text">Negative Sampling</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B0%E7%9A%84%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B0"><span class="nav-number">4.4.1.</span> <span class="nav-text">新的目标函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AF%B9%E4%B8%A4%E7%A7%8D%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%94%B9%E8%BF%9B"><span class="nav-number">4.4.2.</span> <span class="nav-text">对两种模型的改进</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#hierarchical-softmax"><span class="nav-number">4.5.</span> <span class="nav-text">Hierarchical Softmax</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AC%A6%E5%8F%B7%E5%AE%9A%E4%B9%89-2"><span class="nav-number">4.5.1.</span> <span class="nav-text">符号定义</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83%E6%96%B9%E6%B3%95"><span class="nav-number">4.5.2.</span> <span class="nav-text">训练方法</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#word2vec-%E7%9A%84%E8%BE%93%E5%87%BA"><span class="nav-number">4.6.</span> <span class="nav-text">Word2vec 的输出</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%80%9D%E7%BB%B4%E5%AF%BC%E5%9B%BE"><span class="nav-number">5.</span> <span class="nav-text">思维导图</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Zheyu Wang"
      src="/resources/favicon/avatar.png">
  <p class="site-author-name" itemprop="name">Zheyu Wang</p>
  <div class="site-description" itemprop="description">相信过程</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">85</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">28</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">58</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/xxwywzy" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;xxwywzy" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://weibo.com/xxwywzy" title="Weibo → https:&#x2F;&#x2F;weibo.com&#x2F;xxwywzy" rel="noopener me" target="_blank"><i class="fab fa-weibo fa-fw"></i>Weibo</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://twitter.com/xxwywzy" title="Twitter → https:&#x2F;&#x2F;twitter.com&#x2F;xxwywzy" rel="noopener me" target="_blank"><i class="fab fa-twitter fa-fw"></i>Twitter</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://instagram.com/xxwywzy" title="Instagram → https:&#x2F;&#x2F;instagram.com&#x2F;xxwywzy" rel="noopener me" target="_blank"><i class="fab fa-instagram fa-fw"></i>Instagram</a>
      </span>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://xxwywzy.github.io/2019/03/15/cs224-1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/resources/favicon/avatar.png">
      <meta itemprop="name" content="Zheyu Wang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="口仆">
      <meta itemprop="description" content="相信过程">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="CS224N 学习笔记之一：词向量 1 | 口仆">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          CS224N 学习笔记之一：词向量 1
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2019-03-15 15:57:07" itemprop="dateCreated datePublished" datetime="2019-03-15T15:57:07+08:00">2019-03-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" itemprop="url" rel="index"><span itemprop="name">人工智能</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="name">课程笔记</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>4.7k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>16 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><div class="note info"><p>本篇博客为 CS224N 学习笔记第一部分，主题是：词向量（简介，SVD 和 Word2Vec）。</p>
</div>
<span id="more"></span>
<h1 id="自然语言处理简介">自然语言处理简介</h1>
<h2 id="自然语言处理的特殊性">自然语言处理的特殊性</h2>
<p>从处理的对象来看，NLP 与其他机器学习任务有很大区别：NLP 处理的对象是人类语言，而人类的语言是一种特定的用于传达意义的系统，并不由任何形式的物理表现产生，大部分词语只是一个表达某种意义的符号。语言通过各种方式编码（语音、手势、写作等），以连续信号的形式传输给大脑。</p>
<h2 id="任务类型">任务类型</h2>
<p>NLP 的目标是设计算法来让计算机“理解”自然语言，以执行某些任务。这些任务可以划分为不同的难度等级，举例来说：</p>
<p><strong>简单难度</strong>：</p>
<ul>
<li>拼写检查</li>
<li>关键词搜索</li>
<li>同义词寻找</li>
</ul>
<p><strong>中等难度</strong>：</p>
<ul>
<li>从网站、文档中解析信息</li>
</ul>
<p><strong>困难难度</strong>：</p>
<ul>
<li>机器翻译</li>
<li>语义分析</li>
<li>指代消解</li>
<li>智能问答</li>
</ul>
<h2 id="如何表示词语">如何表示词语</h2>
<p>所有 NLP 任务的第一个议题就是如何表示词语以将其作为模型的输入。当前常见的做法是使用<strong>词向量</strong>来表示词语，下面将对各种不同的词向量技术进行介绍。</p>
<h1 id="传统词向量">传统词向量</h1>
<p>最简单的词向量表示方法是 <strong>one-hot 向量</strong>，其步骤如下：</p>
<ol type="1">
<li><p>构建一个大小为 <span class="math inline">\(|V|\)</span> 的词典（如果是表示所有英语词汇，则大小约为 1300 万）</p></li>
<li><p>将每个词语表示为一个 <span class="math inline">\(\mathbb{R}^{|V|\times 1}\)</span> 的向量。该词语在词典中的位置所对应的分量为 1 ，其他分量均为 0：</p>
<p><span class="math display">\[
w^{aardvark} = \left[\begin{array}{cc}
1 \\
0 \\
0 \\
\vdots \\
0
\end{array}\right], w^{a} = \left[\begin{array}{cc}
0 \\
1 \\
0 \\
\vdots \\
0
\end{array}\right], \cdots w^{zebra} = \left[\begin{array}{cc}
0 \\
0 \\
0 \\
\vdots \\
1
\end{array}\right]
\]</span></p></li>
</ol>
<p>这种表示方法的缺陷是无法体现出词语之间的相关性： <span class="math display">\[
(w^{hotel})^Tw^{motel} = (w^{hotel})^Tw^{cat} = 0
\]</span></p>
<p>我们希望减少向量空间的大小，找到一个能够表示词语间相关性的子空间。</p>
<h1 id="基于-svd-的方法">基于 SVD 的方法</h1>
<p>我们可以利用奇异值分解来减小向量空间的大小，具体步骤如下： 1. 基于数据集构建关于词语共现次数的某种矩阵 <span class="math inline">\(X\)</span> 2. 对 <span class="math inline">\(X\)</span> 进行奇异值分解得到 <span class="math inline">\(USV^T\)</span> 3. 使用 <span class="math inline">\(U\)</span> 的某些行来表示词向量</p>
<p>下面介绍两种可以选择的共现矩阵：</p>
<h2 id="词语-文档矩阵">词语-文档矩阵</h2>
<p><strong>词语-文档矩阵</strong>假定相关的词语一般出现在相同的文档中，其构建步骤如下：</p>
<ul>
<li>遍历所有文档（数量为 <span class="math inline">\(M\)</span>）</li>
<li>每一次词语 <span class="math inline">\(i\)</span> 出现在文档 <span class="math inline">\(j\)</span> ，<span class="math inline">\(X_{ij}\)</span> 加 1</li>
</ul>
<p>该矩阵的大小为 <span class="math inline">\(\mathbb{R}^{|V|\times M}\)</span>。这是一个非常大的矩阵，计算过于复杂。</p>
<h2 id="基于窗口的共现矩阵">基于窗口的共现矩阵</h2>
<p><strong>基于窗口的共现矩阵</strong>计算每个词语在给定词语的特定大小窗口范围内出现的次数，矩阵的大小为 <span class="math inline">\(|V| \times |V|\)</span>。</p>
<p>下面给出一个例子：该语料库由 3 个句子组成，且窗口大小设置为 1： 1. I enjoy flying. 2. I like NLP. 3. I like deep learning.</p>
<p>按上述方法得到的矩阵为：</p>
<p><img src="http://media.zjubiomedit.com/2019-03-05-033032.png" width=55%></p>
<h2 id="将-svd-应用到共现矩阵">将 SVD 应用到共现矩阵</h2>
<p>观察 SVD 得到的奇异值（ <span class="math inline">\(S\)</span> 矩阵的对角线），按照所占的百分比选择适当的 <span class="math inline">\(k\)</span>： <span class="math display">\[
\frac {\sum_{i=1}^k \sigma_i} {\sum_{i=1}^{|V|} \sigma_i}
\]</span> 然后选择矩阵 <span class="math inline">\(U\)</span> 的前 <span class="math inline">\(k\)</span> 行作为<strong>词向量</strong>，词向量的长度为 <span class="math inline">\(k\)</span>。该向量能够表示词语的语法和语义信息。</p>
<p>下面两张图给出了 SVD 的求解过程：</p>
<ul>
<li><p>使用 SVD 分解共现矩阵：</p>
<p><img src="http://media.zjubiomedit.com/2019-03-05-055909.png" width=80%></p></li>
<li><p>通过选择前 k 个奇异向量减少维度：</p>
<p><img src="http://media.zjubiomedit.com/2019-03-05-060148.png" width=80%></p></li>
</ul>
<h2 id="存在的问题">存在的问题</h2>
<p>基于 SVD 的方法虽然减小了维数，但是存在很多的问题： + 矩阵维数经常变化（随语料库变化） + 矩阵非常稀疏（因为大部分词语不存在共现） + 矩阵维数一般非常高（<span class="math inline">\(\approx 10^6 \times 10^6\)</span>） + 计算复杂度是平方级的（执行 SVD） + 需要一些技巧来处理词语频率间的极度不平衡</p>
<p>针对上述问题，可以采取如下的解决方案： + 忽略一些功能性词语（如 the、he、has 等） + 使用一个有坡度的窗口（即基于词语之间的距离设置不同的共现权重） + 使用皮尔逊相关性（中心化的余弦相似度）替代原始计数，并将负数置为 0</p>
<p>接下来，我们会介绍一种能更优雅地解决上述诸多问题的方案：<strong>基于迭代</strong>的方法。</p>
<h1 id="基于迭代的方法word2vec">基于迭代的方法：Word2vec</h1>
<p>基于迭代的方法通过迭代逐渐学习词语的共现关系，而非基于 SVD 的方法那样一次性直接获取所有词语的共现关系。训练的过程是：设置一个目标函数，基于某种更新规则进行迭代，不断优化目标函数，最终学习得到词向量。</p>
<p>本节将介绍其中一种方法：Word2vec。Word2vec 是一个软件包，实际包括：</p>
<p>两种算法：<strong>CBOW</strong> 和 <strong>skip-gram</strong></p>
<ul>
<li>CBOW 的目标是基于上下文预测中心词</li>
<li>Skip-gram 的目标是基于中心词预测上下文</li>
</ul>
<p>两种训练方法：<strong>negative sampling</strong> 和 <strong>hierarchical softmax</strong></p>
<ul>
<li>negative sampling 通过采集负样本定义目标函数</li>
<li>hierarchical softmax 通过一个高效的树结构计算所有词语的概率来定义目标函数</li>
</ul>
<h2 id="语言模型">语言模型</h2>
<p>word2vec 可以理解为是语言模型的副产物，语言模型用于计算一个词语序列的概率。如果这个序列是合理的（语义和语法上），其概率就会比较高，否则输出的概率就会比较低。该概率用数学公式可以表示为：</p>
<p><span class="math display">\[
P(w_1,w_2,\ldots,w_n)
\]</span> 如果将每个词语的出现看作完全独立，就可以得到 <strong>Unigram model</strong>： <span class="math display">\[
P(w_1,w_2,\ldots,w_n) = \prod_{i=1}^n P(w_i)
\]</span> 如果假设每个词语的出现于其前一个词语相关，就可以得到 <strong>Bigram model</strong>： <span class="math display">\[
P(w_1,w_2,\ldots,w_n) = \prod_{i=2}^n P(w_i|w_{i-1})
\]</span> 上述两种模型都过于理想化，实际情况下一个词语的出现概率受到更多因素的影响。下面将介绍如何通过模型学习这些概率。</p>
<h2 id="cbow">CBOW</h2>
<p>第一种方法是给定一个单词的上下文，来预测或生成该单词，该模型称为<strong>连续词袋模型</strong>（CBOW）。</p>
<h3 id="符号定义">符号定义</h3>
<p>为了具体描述 CBOW，需要明确以下定义： + <span class="math inline">\(w_i\)</span>：来自词典 <span class="math inline">\(V\)</span> 的词语 <span class="math inline">\(i\)</span> + <span class="math inline">\(\mathcal{V} \in \mathbb{R}^{n\times |V|}\)</span>：输入词语矩阵 + <span class="math inline">\(n\)</span> 是我们希望的词向量（嵌入）空间大小 + <span class="math inline">\(v_i\)</span>：<span class="math inline">\(\mathcal{V}\)</span> 的第 <span class="math inline">\(i\)</span> 列，词语 <span class="math inline">\(w_i\)</span> 的输入向量表示 + <span class="math inline">\(\mathcal{U} \in \mathbb{R}^{|V|\times n}\)</span>：输出词语矩阵 + <span class="math inline">\(u_i\)</span>：<span class="math inline">\(\mathcal{U}\)</span> 的第 <span class="math inline">\(i\)</span> 行，词语 <span class="math inline">\(w_i\)</span> 的输出向量表示</p>
<p>CBOW 的目的是学习得到每一个词语 <span class="math inline">\(w_i\)</span> 的输入向量 <span class="math inline">\(v_i\)</span> 和输出向量 <span class="math inline">\(u_i\)</span>.</p>
<h3 id="计算步骤">计算步骤</h3>
<p>CBOW 的具体步骤如下：</p>
<ol type="1">
<li><p>基于输入上下文（大小为 m）生成 one-hot 词语向量： <span class="math display">\[
x^{(c-m)},\ldots,x^{(c-1)},x^{(c+1)},\dots,x^{(c+m)} \in \mathbb{R}^{|V|}
\]</span></p></li>
<li><p>与输入词语矩阵相乘，得到上下文嵌入词向量： <span class="math display">\[
v_{c-m} = \mathcal{V}x^{(c-m)},v_{c-m+1} = \mathcal{V}x^{(c-m+1)},\ldots,v_{c+m} = \mathcal{V}x^{(c+m)} \in \mathbb{R}^n
\]</span></p>
<ul>
<li>由于 one-hot 向量的性质，得到的词向量即为 <span class="math inline">\(\mathcal{V}\)</span> 的每一列</li>
</ul></li>
<li><p>将这些向量进行平均： <span class="math display">\[
\hat{v} = \frac {v_{c-m}+v_{c-m+1}+\ldots+v_{c+m}} {2m} \in \mathbb{R}^n
\]</span></p>
<ul>
<li>可以理解为用该平均向量表示待预测的词语</li>
</ul></li>
<li><p>生成一个得分向量： <span class="math display">\[
z = \mathcal{U}\hat{v} \in \mathbb{R}^{|V|}
\]</span></p>
<ul>
<li>因为相似的向量的点积更大，所以该得分向量可以反映出每个词语与待预测词语的相似程度</li>
</ul></li>
<li><p>使用 softmax 将得分转换为概率： <span class="math display">\[
\hat{y} = \text{softmax}(z) \in \mathbb{R}^{|V|}
\]</span></p>
<ul>
<li>softmax 的具体形式为 <span class="math inline">\(\frac {e^{z_i}} {\sum_{k=1}^{|V|} e^{z_k}}\)</span></li>
</ul></li>
<li><p>我们希望生成的概率 <span class="math inline">\(\hat{y} \in \mathbb{R}^{|V|}\)</span> 能够匹配真实的概率 <span class="math inline">\(y \in \mathbb{R}^{|V|}\)</span></p>
<ul>
<li>该真实概率实际上就是一个 one-hot 向量（对应到待预测的中心词）</li>
</ul></li>
</ol>
<h3 id="学习方法">学习方法</h3>
<p><img src="http://media.zjubiomedit.com/2019-03-12-070940.png" width=35%></p>
<p>了解到模型的计算方法后，我们希望学习得到 <span class="math inline">\(\mathcal{V}\)</span> 和 <span class="math inline">\(\mathcal{U}\)</span>。我们需要构建一个目标函数来进行优化，这里选择交叉熵函数 <span class="math inline">\(H(\hat{y},y)\)</span>：</p>
<p><span class="math display">\[
H(\hat{y},y) = - \sum_{j=1}^{|V|} y_j \log(\hat{y_j})
\]</span> 由于 <span class="math inline">\(y\)</span> 是一个 one-hot 向量，所以实际的代价函数可以简化为： <span class="math display">\[
H(\hat{y},y) = -y_c \log(\hat{y}_c)
\]</span></p>
<p>其中 <span class="math inline">\(c\)</span> 是中心词的位置（one-hot 向量分量为 1）。因此，综合上述公式，可以得到待优化的目标函数为： <span class="math display">\[
\begin{aligned}
\text{minimize}\;J &amp;= -\log P(w_c | w_{c-m},\ldots, w_{c-1},w_{c+1},\ldots,w_{c+m}) \\
&amp;= -\log P(u_c | \hat{v}) \\
&amp;= -\log \frac {\exp(u_c^T\hat{v})} {\sum_{j=1}^{|V|} \exp(u_j^T\hat{v})} \\
&amp;= -u_c^T\hat{v} + \log \sum_{j=1}^{|V|} \exp(u_j^T\hat{v})
\end{aligned}
\]</span></p>
<p>我们可以使用随机梯度下降来更新所有相关的词向量 <span class="math inline">\(u_c\)</span> 和 <span class="math inline">\(\hat{v}\)</span>： <span class="math display">\[
\mathcal{U}_{new} \leftarrow \mathcal{U}_{old} -\alpha \nabla_{\mathcal{U}} J \\
\mathcal{V}_{new} \leftarrow \mathcal{V}_{old} -\alpha \nabla_{\mathcal{V}} J
\]</span></p>
<h2 id="skip-gram">Skip-Gram</h2>
<p>另一种方法是给定一个中心词，去预测或生成周围的词语，该模型被称为 <strong>Skim-Gram 模型</strong>。</p>
<h3 id="符号定义-1">符号定义</h3>
<p>Skim-Gram 模型的定义与 CBOW 基本相同，只是输入输出对调： - <span class="math inline">\(w_i\)</span>：来自词典 <span class="math inline">\(V\)</span> 的词语 <span class="math inline">\(i\)</span> - <span class="math inline">\(\mathcal{V} \in \mathbb{R}^{n\times |V|}\)</span>：输入词语矩阵 - <span class="math inline">\(n\)</span> 是我们希望的词向量（嵌入）空间大小 - <span class="math inline">\(v_i\)</span>：<span class="math inline">\(\mathcal{V}\)</span> 的第 <span class="math inline">\(i\)</span> 列，词语 <span class="math inline">\(w_i\)</span> 的输入向量表示 - <span class="math inline">\(\mathcal{U} \in \mathbb{R}^{|V|\times n}\)</span>：输出词语矩阵 - <span class="math inline">\(u_i\)</span>：<span class="math inline">\(\mathcal{U}\)</span> 的第 <span class="math inline">\(i\)</span> 行，词语 <span class="math inline">\(w_i\)</span> 的输出向量表示</p>
<h3 id="计算步骤-1">计算步骤</h3>
<p>Skip-Gram 的具体步骤如下：</p>
<ol type="1">
<li><p>生成关于中心词的 one-hot 向量作为输入 <span class="math inline">\(x \in \mathbb{R}^{|V|}\)</span></p></li>
<li><p>与输入词语矩阵相乘，得到中心词的嵌入词向量： <span class="math display">\[
v_c = \mathcal{V}x \in  \mathbb{R}^{n}
\]</span></p></li>
<li><p>生成一个得分向量： <span class="math display">\[
z = \mathcal{U}v_c \in \mathbb{R}^{|V|}
\]</span></p></li>
<li><p>将得分向量转换为概率： <span class="math display">\[
\hat{y} = \text{softmax}(z) \in \mathbb{R}^{|V|}
\]</span></p>
<ul>
<li>其中 <span class="math inline">\(\hat{y}_{c-m},\ldots, \hat{y}_{c-1},\hat{y}_{c+1},\ldots,\hat{y}_{c+m}\)</span> 对应为该中心词的周围词语的概率</li>
</ul></li>
<li><p>我们希望生成的概率与真实的概率 <span class="math inline">\(y^{(c-m)},\ldots, y^{(c-1)},y^{(c+1)},\ldots,y^{(c+m)}\)</span> 匹配（one-hot 向量）</p></li>
</ol>
<h3 id="学习方法-1">学习方法</h3>
<p><img src="http://media.zjubiomedit.com/2019-03-14-064627.png" width=35%></p>
<p>在构建 skip-gram 模型的目标函数时，使用了<strong>贝叶斯假设</strong>（即条件独立假设）：给定中心词的情况下，所有输出词语都完全独立。因此，代价函数如下：</p>
<p><span class="math display">\[
\begin{aligned}
    \text{minimize}\;J &amp;= -\log P( w_{c-m},\ldots, w_{c-1},w_{c+1},\ldots,w_{c+m} | w_c ) \\
    &amp;= -\log \prod_{j=0,j \ne m}^{2m} P(w_{c-m+j} | w_c) \\
    &amp;= -\log \prod_{j=0,j \ne m}^{2m} P(u_{c-m+j} | v_c) \\
    &amp;= -\log \prod_{j=0,j \ne m}^{2m} \frac {\exp(u_{c-m+j}^Tv_c)} {\sum_{k=1}^{|V|} \exp(u_k^Tv_c)} \\
    &amp;= - \sum_{j=0,j \ne m}^{2m} u_{c-m+j}^Tv_c + 2m \log \sum_{k=1}^{|V|} \exp(u_k^Tv_c)
    \end{aligned}
\]</span> 基于上述代价函数，可以通过随机梯度下降更新参数。</p>
<p>注意：代价函数还可以写成如下形式： <span class="math display">\[
\begin{aligned}
J &amp;= - \sum_{j=0,j \ne m}^{2m} \log P(u_{c-m+j}|v_c) \\
&amp;= \sum_{j=0,j \ne m}^{2m} H(\hat{y},y_{c-m+j})
\end{aligned}
\]</span></p>
<ul>
<li>其中 <span class="math inline">\(H(\hat{y},y_{c-m+j})\)</span> 是概率向量 <span class="math inline">\(\hat{y}\)</span> 和 one-hot 向量 <span class="math inline">\(y_{c-m+j}\)</span> 的交叉熵</li>
</ul>
<h2 id="negative-sampling">Negative Sampling</h2>
<p>对于上述的两种模型，其代价函数需要在 <span class="math inline">\(|V|\)</span> 上进行累加，其时间复杂度为 <span class="math inline">\(O(|V|)\)</span>。为了减少计算复杂度，在每一次迭代中，我们可以仅对部分<strong>负样本</strong>进行采样，采样基于一个噪声分布 <span class="math inline">\(P_n(w)\)</span> 进行，其概率与词典中词频顺序匹配。Gensim 默认使用 CBOW 和负采样。</p>
<h3 id="新的目标函数">新的目标函数</h3>
<p>实际上，负采样在优化一个不同的目标函数。考虑一个由一个词语和一个上下文组成的单词对：<span class="math inline">\((w,c)\)</span>，现在我们考虑的是该单词对是否出自训练数据：</p>
<ul>
<li>定义 <span class="math inline">\(P(D=1|w,c)\)</span> 为 <span class="math inline">\((w,c)\)</span> 出自语料库的概率</li>
<li>定义 <span class="math inline">\(P(D=0|w,c)\)</span> 为 <span class="math inline">\((w,c)\)</span> 不出自语料库的概率</li>
</ul>
<p>我们使用 sigmoid 函数来定义 <span class="math inline">\(P(D=1|w,c)\)</span>，sigmoid 可以理解为 1 维版本的 softmax 函数： <span class="math display">\[
P(D=1|w,c) = \sigma(u_w^Tv_c) = \frac 1 {1+e^{(-u_w^Tv_c)}}
\]</span></p>
<p>新的目标函数希望最大化上述两个概率：当单词对实际上出自语料库时，最大化 <span class="math inline">\(P(D=1|w,c)\)</span>；当单词对实际上并不出自语料库时，最大化 <span class="math inline">\(P(D=0|w,c)\)</span>，具体公式如下：</p>
<p><span class="math display">\[
\begin{aligned}
\theta &amp;= \arg\max_\theta \prod_{(w,c)\in D} P(D=1|w,c,\theta) \prod_{(w,c)\in \tilde{D}} P(D=0|w,c,\theta) \\
&amp;= \arg\max_\theta \prod_{(w,c)\in D} P(D=1|w,c,\theta) \prod_{(w,c)\in \tilde{D}} (1-P(D=1|w,c,\theta)) \\
&amp;= \arg\max_\theta \sum_{(w,c)\in D} \log P(D=1|w,c,\theta) + \sum_{(w,c)\in \tilde{D}} \log (1-P(D=1|w,c,\theta)) \\
&amp;= \arg\max_\theta \sum_{(w,c)\in D} \log  \frac 1 {1+\exp(-u_w^Tv_c)}  + \sum_{(w,c)\in \tilde{D}} \log (1- \frac 1 {1+\exp(-u_w^Tv_c)}) \\
&amp;= \arg\max_\theta \sum_{(w,c)\in D} \log  \frac 1 {1+\exp(-u_w^Tv_c)}  + \sum_{(w,c)\in \tilde{D}} \log (\frac 1 {1+\exp(u_w^Tv_c)}) \\
\end{aligned}
\]</span></p>
<ul>
<li>公式中使用 <span class="math inline">\(\theta\)</span> 表示模型的参数，实际上参数为 <span class="math inline">\(\mathcal{V}\)</span> 和 <span class="math inline">\(\mathcal{U}\)</span></li>
</ul>
<p>最大化似然函数与最小化负的似然函数等价，因此可以得到如下代价函数： <span class="math display">\[
J = - \sum_{(w,c)\in D} \log  \frac 1 {1+\exp(-u_w^Tv_c)} - \sum_{(w,c)\in \tilde{D}} \log (\frac 1 {1+\exp(u_w^Tv_c)})
\]</span></p>
<p>其中 <span class="math inline">\(\tilde{D}\)</span> 指负语料库，即在实际情况中不太可能出现的句子，一般我们使用基于某个噪声分布随机采样的方式生成 <span class="math inline">\(\tilde{D}\)</span>。</p>
<h3 id="对两种模型的改进">对两种模型的改进</h3>
<p>对于 skip-gram，基于中心词 <span class="math inline">\(c\)</span> 的某个上下文单词 <span class="math inline">\(c-m+j\)</span> 对应的代价函数为： <span class="math display">\[
- u_{c-m+j}^Tv_c + \log \sum_{k=1}^{|V|} \exp(u_k^Tv_c)
\]</span></p>
<p>基于负采样的<strong>改进后</strong>的代价函数为： <span class="math display">\[
- \log \sigma(u_{c-m+j}^T v_c) - \sum_{k=1}^{K} \log \sigma(-\tilde{u}_k^Tv_c)
\]</span> 对于 CBOW，给定上下文向量 <span class="math inline">\(\hat{v}\)</span> 的中心词 <span class="math inline">\(u_c\)</span> 对应的代价函数为： <span class="math display">\[
-u_c^T\hat{v} + \log \sum_{j=1}^{|V|} \exp(u_j^T\hat{v})
\]</span></p>
<p>基于负采样的<strong>改进后</strong>的代价函数为： <span class="math display">\[
- \log \sigma(u_{c}^T \hat{v}) - \sum_{k=1}^{K} \log \sigma(-\tilde{u}_k^T\hat{v})
\]</span> 在上述公式中，<span class="math inline">\(\{\tilde{u}_k | k=1\ldots k\}\)</span> 为负样本，随机采样自 <span class="math inline">\(P_n(w)\)</span>。文献中指出目前最有效的 <span class="math inline">\(P_n(w)\)</span> 是 Unigram Model 的 <span class="math inline">\(3/4\)</span> 次幂。</p>
<h2 id="hierarchical-softmax">Hierarchical Softmax</h2>
<p>另一种优化代价函数计算的方法是 <strong>hierarchical softmax</strong>。在实际应用中，hierarchical softmax 对低频词的效果更好，而负采样对常用词和低维词向量的效果更好。</p>
<p>Hierarchical softmax 使用一个<strong>二叉树</strong>来表示词典中的所有词语。每个叶子节点都是一个词语，从根节点到叶子节点的路径唯一：</p>
<p><img src="http://media.zjubiomedit.com/2019-03-15-032628.jpg" width=50%></p>
<p>在 Hierarchical softmax 中，给定一个词向量 <span class="math inline">\(w_i\)</span> ，一个词语 <span class="math inline">\(w\)</span> 的概率 <span class="math inline">\(P(w|w_i)\)</span> 等价于从根节点出发随机行走至 <span class="math inline">\(w\)</span> 对应的叶子节点的概率。这样表示优点是其<strong>计算复杂度</strong>为 <span class="math inline">\(O((\log(|V|))\)</span>.</p>
<p>在该训练方法中，<strong>没有词语的输出表达</strong>（即 <span class="math inline">\(\mathcal{U}\)</span> 矩阵）。取而代之的是每个节点（叶子节点除外）代表一个向量，模型需要去学习这些向量。</p>
<h3 id="符号定义-2">符号定义</h3>
<p>为了更好地描述模型，给出如下定义： + <span class="math inline">\(L(w)\)</span>：从根节点到叶子节点 <span class="math inline">\(w\)</span> 的节点数量（包括根节点但不包括叶子节点） + 例如上图中，<span class="math inline">\(L(w_2)\)</span> 为 3 + <span class="math inline">\(n(w,i)\)</span>：在该路径上的第 <span class="math inline">\(i\)</span> 个节点，对应的向量为 <span class="math inline">\(v_{n(w,i)}\)</span> + <span class="math inline">\(n(w,1)\)</span> 是根节点，<span class="math inline">\(n(w, L(w))\)</span> 是 <span class="math inline">\(w\)</span> 的父节点 + 对于每个内部节点 <span class="math inline">\(n\)</span>，我们以某种规则选择其的一个孩子节点（二选一），称为 <span class="math inline">\(ch(n)\)</span> + 例如总是选择左边的孩子节点</p>
<h3 id="训练方法">训练方法</h3>
<p>基于上述定义，<span class="math inline">\(P(w|w_i)\)</span> 可以表示为： <span class="math display">\[
P(w|w_i) = \prod_{j=1}^{L(w)-1} \sigma ([n(w,j+1) = ch(n(w,j))] \cdot v_{n(w,j)}^Tv_{w_i})
\]</span></p>
<p>其中： <span class="math display">\[
[x] = \left\{
\begin{aligned}
1 \;\text{if x is true}\\
-1 \;\text{otherwise}
\end{aligned}
\right.
\]</span> 下面对上述公式进行解释：</p>
<p>首先，我们基于从根节点 <span class="math inline">\(n(w,1)\)</span> 到叶子节点 <span class="math inline">\(n(w)\)</span> 的路径计算向量的点积。 注意 <span class="math inline">\(v_{n(w,j)}\)</span> 是节点词语对应的向量，<span class="math inline">\(v_{w_i}\)</span> 则是基于输入矩阵得到的根节点词语的向量，两者并不相关，都需要学习。</p>
<p>然后，通过 sigmoid 函数得到每个节点的概率输出。<span class="math inline">\([n(w,j+1) = ch(n(w,j))]\)</span> 保证了选择左边子节点和右边子节点的概率之和为 1： <span class="math display">\[
\sigma(v_n^Tv_{w_i}) + \sigma(-v_n^Tv_{w_i}) = 1
\]</span></p>
<ul>
<li>同时也保证了 <span class="math inline">\(\sum_{w=1}^{|V|} P(w|w_i) =1\)</span>（将概率沿树向上累加可以证明）</li>
</ul>
<p>最后，将节点概率相乘得到输出。以图中的 <span class="math inline">\(w_2\)</span> 为例，其概率为：</p>
<p><span class="math display">\[
\begin{aligned}
P(w_2|w_i) &amp;= p(n(w_2,1),\text{left}) \cdot p(n(w_2,2),\text{left}) \cdot p(n(w_2,3),\text{right}) \\
&amp;= \sigma (v^T_{n(w_2,1)}v_{w_i})\cdot\sigma (v^T_{n(w_2,2)}v_{w_i})\cdot\sigma (-v^T_{n(w_2,3)}v_{w_i})
\end{aligned}
\]</span> 为了训练模型，我们的目标依然是最小化负对数似然函数 <span class="math inline">\(-\log P(w|w_i)\)</span>，学习的参数为<strong>节点对应的向量</strong>以及<strong>输入矩阵</strong>。该方法的速度取决于二叉树的构建，原论文使用了一个二叉霍夫曼树（给予常用词更短的路径）。</p>
<h2 id="word2vec-的输出">Word2vec 的输出</h2>
<p>不管使用哪种模型或训练方法，最终都会得到训练完成后的输入矩阵 <span class="math inline">\(\mathcal{V}\)</span>。我们所需要的词向量通过 one-hot 向量与输入矩阵相乘得到（即为模型的隐藏层）： <span class="math display">\[
v_{output} = \mathcal{V}x_{input}
\]</span></p>
<h1 id="思维导图">思维导图</h1>
<p><img src="http://media.zjubiomedit.com/2019-03-15-085129.png" width=100%></p>

    </div>

    
    
    

    <footer class="post-footer">




<div class="license">
  <div class="license-title">CS224N 学习笔记之一：词向量 1</div>
  <div class="license-link">
    <a href="https://xxwywzy.github.io/2019/03/15/cs224-1/">https://xxwywzy.github.io/2019/03/15/cs224-1/</a>
  </div>
  <div class="license-meta">
    <div class="license-meta-item">
      <div class="license-meta-title">本文作者</div>
      <div class="license-meta-text">
          Zheyu Wang
      </div>
    </div>
      <div class="license-meta-item">
        <div class="license-meta-title">发布于</div>
        <div class="license-meta-text">
          2019-03-15
        </div>
      </div>
      <div class="license-meta-item">
        <div class="license-meta-title">更新于</div>
        <div class="license-meta-text">
          2023-08-05
        </div>
      </div>
    <div class="license-meta-item">
      <div class="license-meta-title">许可协议</div>
      <div class="license-meta-text">
          <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" rel="noopener" target="_blank">CC BY-NC-SA 4.0</a>
      </div>
    </div>
  </div>
  <div class="license-statement">
      转载或引用本文时，请遵守上述许可协议，注明出处、不得用于商业用途！
  </div>
</div>
          <div class="post-tags">
              <a href="/tags/CS224N/" rel="tag"># CS224N</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2019/03/13/pl-1-hw/" rel="prev" title="Progamming Languages 第一部分作业">
                  <i class="fa fa-angle-left"></i> Progamming Languages 第一部分作业
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2019/03/20/algorithm-1/" rel="next" title="《算法（第4版）》第一章读书笔记：基础编程模型">
                  《算法（第4版）》第一章读书笔记：基础编程模型 <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments" id="lv-container" data-id="city" data-uid="MTAyMC81ODgyNi8zNTI4OA=="></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2023</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Zheyu Wang</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
      <span>站点总字数：</span>
    <span title="站点总字数">332k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span>站点阅读时长 &asymp;</span>
    <span title="站点阅读时长">18:26</span>
  </span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  






  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<script src="/js/third-party/comments/livere.js"></script>



  <style>
    #taboola-livere { display: none;}
  </style>



<script type="text/javascript">
var linkLists = document.querySelectorAll(".link-list");

linkLists.forEach(function(linkList) {
  var listPath = linkList.getAttribute('json-src');
  var iconPath = linkList.getAttribute('icon-src');
  
  var xhr = new XMLHttpRequest();
  xhr.open('GET', listPath, true);
  xhr.onreadystatechange = function() {
    if (xhr.readyState === 4 && xhr.status === 200) {
      var data = JSON.parse(xhr.responseText);
      
      var li = "";
      linkList.innerHTML = '';

      for (var infoIndex = 0; infoIndex < data.length; infoIndex++) {
        var info = data[infoIndex];
        var labelWarn = info['warn'] ? '<span class="label warn">' + info['warn'] + '</span>' : '';
        var labelInfo = info['info'] ? '<span class="label info">' + info['info'] + '</span>' : '';

        li += '<div class="link-list-container">';
        li += '<img class="link-list-image" src="' + iconPath + info['logo'] + '">';
        li += '<p>' + info['title'] + labelInfo + labelWarn + '</p>';
        li += '<p>' + info['intro'] + '</p>';
        li += '<a href="' + info['url'] + '" rel="noopener" target="_blank" data-pjax-state=""></a>';
        li += '</div>';
      }
      
      linkList.innerHTML = li;
    }
  };
  xhr.send();
});
</script>


<script type="text/javascript">
var cultureList = document.querySelectorAll(".culture-list");
if (cultureList.length !== 0) {
  var j = -1;
  for (var i = 0; i < cultureList.length; i++) {
    const listPath = cultureList[i].getAttribute('json-src');
    const coverPath = cultureList[i].getAttribute('cover-src');
    
    var xhr = new XMLHttpRequest();
    xhr.open('GET', listPath, true);
    xhr.onreadystatechange = function () {
      if (xhr.readyState === 4 && xhr.status === 200) {
        j++;
        var data = JSON.parse(xhr.responseText);
        var li = "";
        
        cultureList[j].innerHTML = '';

        for (var infoIndex = 0; infoIndex < data.length; infoIndex++) {
          var info = data[infoIndex];
          
          var title = info['title'];
          if (info['link']) {
            title = '<a href="' + info['link'] + '">' + info['title'] + '</a>';
          }

          var author = info['author'] ? '<span class="author">' + info['author'] + '</span>' : '';

          var intro = info['intro'] ? info['intro'] : '';

          var star = '';
          if (info['score'] == null) {
            star = '';
          } else {
            var colorStar = '';
            var greyStar = '';
            var int = Math.floor(info['score']); //整数部分
            var fract = 0;
            if (info['score'] % 1 !== 0) {
              fract = 1;
            }
            for (var m = 0; m < int; m++) {
              colorStar += '★';
            }
            if (fract !== 0) {
              colorStar += '☆';
            }
            for (var m = 0; m < (5 - fract - int); m++) {
              greyStar += '☆';
            }
            if (info['score'] !== 5) {
              star = '<span class="star-score">' + colorStar + '<span class="grey-star">' + greyStar + '</span></span>';
            } else {
              star = '<span class="star-score">' + colorStar + '</span>';
            }
          }

          li += '<div class="media">';
          li += '<div class="media-cover" style="background-image:url(' + coverPath + info['cover'] + ')"></div>';
          li += '<div class="media-meta">';
          li += '<div class="media-meta-item title">' + title + '</div>';
          li += '<div class="media-meta-item">' + author + star + '</div>';
          li += '<div class="media-meta-item intro">' + intro + '</div>';
          li += '</div></div>';
        }
        
        cultureList[j].innerHTML = li;
      }
    };
    xhr.send();
  }
}
</script>




<script src="/resources/minigrid.min.js"></script>
<script type="text/javascript">
var album = document.querySelector(".album");
if (album) {
  // 相册列表 JSON 数据
  var imgDataPath = album.getAttribute('json-src');
  // 照片存储路径
  var imgPath = album.getAttribute('photo-src');
  // 最多显示数量
  var imgMaxNum = 50;
  // 获取窗口大小以决定图片宽度
  var windowWidth = window.innerWidth || document.documentElement.clientWidth || document.body.clientWidth;
  var imageWidth;

  if (windowWidth < 768) {
    imageWidth = 145; // 移动端图片宽度
  } else {
    imageWidth = 235;
  }

  // 腾讯云自定义样式 (数据万象外网流量需要付费)
  //var imgStyle = '!' + imageWidth + 'x';
  //var imgStyle = '!300x';

  // 生成相册
  var linkDataPath = imgDataPath;
  var photo = {
    page: 1,
    offset: imgMaxNum,
    init: function () {
      var that = this;
      var xhr = new XMLHttpRequest();
      xhr.open("GET", linkDataPath, true);
      xhr.onreadystatechange = function () {
        if (xhr.readyState === 4 && xhr.status === 200) {
          var data = JSON.parse(xhr.responseText);
          that.render(that.page, data);
        }
      };
      xhr.send();
    },
    render: function (page, data) {
      var begin = (page - 1) * this.offset;
      var end = page * this.offset;
      if (begin >= data.length) return;
      var imgNameWithPattern, imgName, imageSize, imageX, imageY, li = "";
      for (var i = begin; i < end && i < data.length; i++) {
        imgNameWithPattern = data[i].split(' ')[1];
        imgName = imgNameWithPattern.split('.')[0];
        imageSize = data[i].split(' ')[0];
        imageX = imageSize.split('.')[0];
        imageY = imageSize.split('.')[1];
        li += '<div class="card" style="width:' + imageWidth + 'px" >';
        li += '<div class="album-photo" style="height:'+ imageWidth * imageY / imageX + 'px">';
        li += '<a class="fancybox fancybox.image" href="' + imgPath + imgNameWithPattern + '" itemscope="" itemtype="http://schema.org/ImageObject" itemprop="url" data-fancybox="group" rel="group" data-caption="' + imgName + '" title="' +  imgName + '">';
        li += '<img data-src="' + imgPath + imgNameWithPattern + '" src="' + imgPath + imgNameWithPattern + '" alt="' +  imgName + '" data-loaded="true">';
        li += '</a>';
        li += '</div>';
        li += '</div>';
      }
      album.insertAdjacentHTML('beforeend', li);
      this.minigrid();
    },
    minigrid: function () {
      var grid = new Minigrid({
        container: '.album',
        item: '.card',
        gutter: 12
      });
      grid.mount();
      window.addEventListener('resize', function () {
        grid.mount();
      });
    }
  };
  photo.init();
}
</script>
</body>
</html>
