<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/resources/favicon/favicon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/resources/favicon/favicon.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/resources/favicon/favicon.png">
  <link rel="mask-icon" href="/resources/favicon/favicon.png" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css" integrity="sha256-CTSx/A06dm1B063156EVh15m6Y67pAjZZaQc89LLSrU=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"xxwywzy.github.io","root":"/","images":"/resources/img/","scheme":"Gemini","darkmode":true,"version":"8.18.2","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":true,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"livere","storage":true,"lazyload":false,"nav":null,"activeClass":"livere"},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="本篇博客为对 LDA 的原理解读第一篇，主要参考网上流传已久的文章《LDA 数学八卦》。">
<meta property="og:type" content="article">
<meta property="og:title" content="LDA 原理第一部分：LDA 数学八卦">
<meta property="og:url" content="https://xxwywzy.github.io/2019/08/26/lda-1/">
<meta property="og:site_name" content="口仆">
<meta property="og:description" content="本篇博客为对 LDA 的原理解读第一篇，主要参考网上流传已久的文章《LDA 数学八卦》。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://media.zjubiomedit.com/2019-08-16-122049.png">
<meta property="og:image" content="http://media.zjubiomedit.com/2019-08-26-032341.jpg">
<meta property="og:image" content="http://media.zjubiomedit.com/2019-08-30-075754.jpg">
<meta property="og:image" content="http://media.zjubiomedit.com/2019-09-02-022004.jpg">
<meta property="og:image" content="http://media.zjubiomedit.com/2019-09-02-024609.jpg">
<meta property="og:image" content="http://media.zjubiomedit.com/2019-09-04-075810.jpg">
<meta property="og:image" content="http://media.zjubiomedit.com/2019-09-05-013456.jpg">
<meta property="og:image" content="http://media.zjubiomedit.com/2019-09-05-013847.jpg">
<meta property="og:image" content="http://media.zjubiomedit.com/2019-09-05-021751.jpg">
<meta property="og:image" content="http://media.zjubiomedit.com/2019-09-05-084257.jpg">
<meta property="article:published_time" content="2019-08-26T07:53:17.000Z">
<meta property="article:modified_time" content="2023-08-05T08:58:01.000Z">
<meta property="article:author" content="Zheyu Wang">
<meta property="article:tag" content="LDA">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://media.zjubiomedit.com/2019-08-16-122049.png">


<link rel="canonical" href="https://xxwywzy.github.io/2019/08/26/lda-1/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://xxwywzy.github.io/2019/08/26/lda-1/","path":"2019/08/26/lda-1/","title":"LDA 原理第一部分：LDA 数学八卦"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>LDA 原理第一部分：LDA 数学八卦 | 口仆</title>
  











<link rel="stylesheet" href="/resources/fonts/longcang/longcang-regular.css" >
<link rel="stylesheet" href="/resources/fonts/lxgw/lxgwwenkailite-regular.css" >
  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">口仆</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">Long may the sunshine</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-culture"><a href="/culture/" rel="section"><i class="fa fa-heartbeat fa-fw"></i>MEME</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#gamma-%E5%87%BD%E6%95%B0"><span class="nav-number">1.</span> <span class="nav-text">Gamma 函数</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#betadirichlet-%E5%88%86%E5%B8%83"><span class="nav-number">2.</span> <span class="nav-text">Beta&#x2F;Dirichlet 分布</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#mcmc-%E5%92%8C-gibbs-sampling"><span class="nav-number">3.</span> <span class="nav-text">MCMC 和 Gibbs Sampling</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#markov-chain-monte-carlo"><span class="nav-number">3.1.</span> <span class="nav-text">Markov Chain Monte Carlo</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#gibbs-sampling"><span class="nav-number">3.2.</span> <span class="nav-text">Gibbs Sampling</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%96%87%E6%9C%AC%E5%BB%BA%E6%A8%A1"><span class="nav-number">4.</span> <span class="nav-text">文本建模</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#unigram-model"><span class="nav-number">4.1.</span> <span class="nav-text">Unigram Model</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#plsa-model"><span class="nav-number">4.2.</span> <span class="nav-text">PLSA Model</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#lda-%E6%96%87%E6%9C%AC%E5%BB%BA%E6%A8%A1"><span class="nav-number">5.</span> <span class="nav-text">LDA 文本建模</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%89%A9%E7%90%86%E8%BF%87%E7%A8%8B%E5%88%86%E8%A7%A3"><span class="nav-number">5.1.</span> <span class="nav-text">物理过程分解</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#m-%E4%B8%AA-dirichlet-multinomial-%E5%85%B1%E8%BD%AD%E7%BB%93%E6%9E%84"><span class="nav-number">5.1.1.</span> <span class="nav-text">M 个 Dirichlet-Multinomial 共轭结构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#k-%E4%B8%AA-dirichlet-multinomial-%E5%85%B1%E8%BD%AD%E7%BB%93%E6%9E%84"><span class="nav-number">5.1.2.</span> <span class="nav-text">K 个 Dirichlet-Multinomial 共轭结构</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#gibbs-sampling-1"><span class="nav-number">5.2.</span> <span class="nav-text">Gibbs Sampling</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#training-and-inference"><span class="nav-number">5.3.</span> <span class="nav-text">Training and Inference</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Zheyu Wang"
      src="/resources/favicon/avatar.png">
  <p class="site-author-name" itemprop="name">Zheyu Wang</p>
  <div class="site-description" itemprop="description">相信过程</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">85</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">28</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">58</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/xxwywzy" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;xxwywzy" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://weibo.com/xxwywzy" title="Weibo → https:&#x2F;&#x2F;weibo.com&#x2F;xxwywzy" rel="noopener me" target="_blank"><i class="fab fa-weibo fa-fw"></i>Weibo</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://twitter.com/xxwywzy" title="Twitter → https:&#x2F;&#x2F;twitter.com&#x2F;xxwywzy" rel="noopener me" target="_blank"><i class="fab fa-twitter fa-fw"></i>Twitter</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://instagram.com/xxwywzy" title="Instagram → https:&#x2F;&#x2F;instagram.com&#x2F;xxwywzy" rel="noopener me" target="_blank"><i class="fab fa-instagram fa-fw"></i>Instagram</a>
      </span>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://xxwywzy.github.io/2019/08/26/lda-1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/resources/favicon/avatar.png">
      <meta itemprop="name" content="Zheyu Wang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="口仆">
      <meta itemprop="description" content="相信过程">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="LDA 原理第一部分：LDA 数学八卦 | 口仆">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          LDA 原理第一部分：LDA 数学八卦
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2019-08-26 15:53:17" itemprop="dateCreated datePublished" datetime="2019-08-26T15:53:17+08:00">2019-08-26</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" itemprop="url" rel="index"><span itemprop="name">人工智能</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E6%96%87%E7%AB%A0%E7%B2%BE%E8%AF%BB/" itemprop="url" rel="index"><span itemprop="name">文章精读</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>7.4k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>25 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><div class="note info"><p>本篇博客为对 LDA 的原理解读第一篇，主要参考网上流传已久的文章《LDA 数学八卦》。</p>
</div>
<span id="more"></span>
<h1 id="gamma-函数">Gamma 函数</h1>
<p>作者首先介绍了 Gamma 函数： <span class="math display">\[
\Gamma(x)=\int_{0}^{\infty} t^{x-1} e^{-t} d t
\]</span> Gamma 函数在数学上应用广泛，它可以将阶乘延拓到实数集上： <span class="math display">\[
\Gamma(n)=(n-1) !
\]</span> 在概率统计中，Gamma 函数也频繁出现。首先，由 Gamma 函数可以推导出 Gamma 分布： <span class="math display">\[
\operatorname{Gamma}(x | \alpha)=\frac{x^{\alpha-1} e^{-x}}{\Gamma(\alpha)}
\]</span> Gamma 分布和 Poisson 分布有着密切的联系： + 参数为 <span class="math inline">\(\lambda\)</span> 的 Poisson 分布，概率写为： <span class="math display">\[
\text {Poisson }(X=k | \lambda)=\frac{\lambda^{k} e^{-\lambda}}{k !}
\]</span></p>
<ul>
<li>而 Gamma 分布中取 <span class="math inline">\(\alpha = k+1\)</span> 得到： <span class="math display">\[
\operatorname{Gamma}(x | \alpha=k+1)=\frac{x^{k} e^{-x}}{\Gamma(k+1)}=\frac{x^{k} e^{-x}}{k !}
\]</span></li>
</ul>
<p>这两个分布在数学形式上是一致的，可以直观地认为 Gamma 分布是 Poisson 分布在正实数集上的连续化版本。</p>
<p>我们可以从二项分布出发，利用其极限是 Poisson 分布的性质，将 Gamma 分布和 Poisson 分布联系起来： <span class="math display">\[
\text {Poisson }(X \leq k | \lambda)+\int_{0}^{\lambda} \frac{x^{k} e^{-x}}{k !} d x=1
\]</span> 可以看到，两者的概率累积函数有互补的关系。</p>
<h1 id="betadirichlet-分布">Beta/Dirichlet 分布</h1>
<p>原文中作者通过一个游戏介绍了各种分布，这里省略过程直接给出重要的结论：</p>
<p>Beta 分布的概率函数为： <span class="math display">\[
f(x)=\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha) \Gamma(\beta)} x^{\alpha-1}(1-x)^{\beta-1}
\]</span> 其中 <span class="math inline">\(\alpha=k, \beta=n-k+1\)</span></p>
<p>Beta 分布是一种定义区间为 [0,1] 上的连续概率分布，其概率密度图像（某一点的值没有意义）计算如下：</p>
<p><img src="http://media.zjubiomedit.com/2019-08-16-122049.png" width=40%></p>
<p>在贝叶斯分析的过程下，可以证明 <strong>Beta 分布和二项分布共轭</strong>： <span class="math display">\[
\operatorname{Beta}(p | \alpha, \beta)+\text {Binom}\left(m_{1}, m_{2}\right)=\operatorname{Beta}\left(p | \alpha+m_{1}, \beta+m_{2}\right)
\]</span> 共轭的意思就是：数据符合二项分布的时候，参数的先验分布和后验分布都能保持 Beta 分布的形态，这样便于我们在先验分布中赋予参数很明确的物理意义，这个物理意义可以延续到后验分布中进行解释。</p>
<p>类似地，Dirichlet 分布是 Beta 分布在高维上的推广： <span class="math display">\[
\operatorname{Dir}(\vec{p} | \vec{\alpha})=\frac{\Gamma\left(\sum_{k=1}^{K} \alpha_{k}\right)}{\prod_{k=1}^{K} \Gamma\left(\alpha_{k}\right)} \prod_{k=1}^{K} p_{k}^{\alpha_{k}-1}
\]</span> 可以证明，<strong>Dirichlet 分布与多项分布共轭</strong>： <span class="math display">\[
\operatorname{Dir}(\vec{p} | \vec{\alpha})+\operatorname{Mult}(\vec{m})=\operatorname{Dir}(\vec{p} | \vec{\alpha}+\vec{m})
\]</span> Beta/ Dirichlet 分布具有如下的性质：</p>
<p>如果 <span class="math inline">\(p \sim \operatorname{Beta}(t | \alpha, \beta)\)</span>，则： <span class="math display">\[
E(p) =\frac{\alpha}{\alpha+\beta} 
\]</span> 如果 <span class="math inline">\(\vec{p} \sim \operatorname{Dir}(\vec{t} | \vec{\alpha})\)</span>，则： <span class="math display">\[
E(\vec{p})=\left(\frac{\alpha_{1}}{\sum_{i=1}^{K} \alpha_{i}}, \frac{\alpha_{2}}{\sum_{i=1}^{K} \alpha_{i}}, \cdots, \frac{\alpha_{K}}{\sum_{i=1}^{K} \alpha_{i}}\right)
\]</span></p>
<h1 id="mcmc-和-gibbs-sampling">MCMC 和 Gibbs Sampling</h1>
<p>在随机模拟方法（又称为蒙特卡洛方法）中，一个重要问题是给定一个概率分布 <span class="math inline">\(p(x)\)</span>，如何在计算机中生成它的样本。</p>
<p>一般而言，均匀分布 <span class="math inline">\(Uniform(0,1)\)</span> 的样本是相对容易生成的，而常见的概率分布可以通过均匀分布变换得到。然而，当 <span class="math inline">\(p(x)\)</span> 的形式很复杂，或 <span class="math inline">\(p(x)\)</span> 是高维分布的时候，样本的生成就很困难了。</p>
<p>此时就需要一些更加复杂的随机模拟的方法来生成样本。本节将介绍两种常用的方法：MCMC（Markov Chain Monte Carlo）和 Gibbs Sampling 方法。</p>
<p>首先，我们需要了解马尔科夫链的<strong>平稳分布</strong>性质。</p>
<p>马氏链的数学定义如下： <span class="math display">\[
P\left(X_{t+1}=x | X_{t}, X_{t-1}, \cdots\right)=P\left(X_{t+1}=x | X_{t}\right)
\]</span> 即状态转移的概率只依赖于前一个状态。</p>
<p>我们可以证明，迭代次数足够大时，马氏链（非周期，可无限状态且状态相通）将收敛到一个稳定的分布，该分布与初始状态无关，仅与概率转移矩阵有关。收敛后得到的变量均符合该分布，但并<strong>不独立</strong>。</p>
<p>该分布 <span class="math inline">\(\pi\)</span> 称为马氏链的<strong>平稳分布</strong>。<span class="math inline">\(\pi\)</span> 是方程 <span class="math inline">\(\pi P = \pi\)</span> 的唯一非负解。</p>
<h2 id="markov-chain-monte-carlo">Markov Chain Monte Carlo</h2>
<p>对于给定的概率分布 <span class="math inline">\(p(x)\)</span>，如果我们能构造一个转移矩阵为 <span class="math inline">\(P\)</span> 的马氏链，使得该马氏链的平稳分布恰好是 <span class="math inline">\(p(x)\)</span>，那么该马氏链收敛后的序列都将是 <span class="math inline">\(p(x)\)</span> 的样本。现在的问题是，我们如何构造转移矩阵 <span class="math inline">\(P\)</span>，使得平稳分布恰好是我们要的分布 <span class="math inline">\(p(x)\)</span> 呢？</p>
<p>我们将使用下述定理来进行构造：</p>
<ul>
<li><p>细致平稳条件：如果非周期马氏链的转移矩阵 <span class="math inline">\(P\)</span> 和分布 <span class="math inline">\(\pi(x)\)</span> 满足： <span class="math display">\[
\pi(i) P_{i j}=\pi(j) P_{j i} \quad \text { for all } \quad i, j
\]</span></p>
<ul>
<li>则 <span class="math inline">\(\pi(x)\)</span> 是马氏链的平稳分布</li>
</ul></li>
<li><p>对于一个转移矩阵为 <span class="math inline">\(q(i,j)\)</span> 的马氏链，通常情况下，细致平稳条件不成立： <span class="math display">\[
p(i) q(i, j) \neq p(j) q(j, i)
\]</span></p></li>
<li><p>我们可以引入一个 <span class="math inline">\(\alpha(i,j)\)</span> ，使得： <span class="math display">\[
p(i) q(i, j) \alpha(i, j)=p(j) q(j, i) \alpha(j, i)
\]</span></p></li>
<li><p>我们可以取： <span class="math display">\[
\alpha(i, j)=p(j) q(j, i) \quad \alpha(j, i)=p(i) q(i, j)
\]</span></p></li>
<li><p>于是有： <span class="math display">\[
p(i) \underbrace{q(i, j) \alpha(i, j)}_{Q^{\prime}(i, j)}=p(j) \underbrace{q(j, i) \alpha(j, i)}_{Q^{\prime}(j, i)}
\]</span></p></li>
<li><p>于是我们把原来的马氏链改造成了具有转移矩阵 <span class="math inline">\(Q^\prime\)</span> 的马氏链，其平稳分布即为 <span class="math inline">\(p(x)\)</span>！</p></li>
</ul>
<p>在改造 <span class="math inline">\(Q\)</span> 的过程中引入的 <span class="math inline">\(\alpha(i,j)\)</span> 称为接受率，可以理解为在原来的马氏链上，从状态 <span class="math inline">\(i\)</span> 以 <span class="math inline">\(q(i,j)\)</span> 的概率转跳到状态 <span class="math inline">\(j\)</span> 的时候，我们以 <span class="math inline">\(\alpha(i,j)\)</span> 的概率接受这个转移：</p>
<p><img src="http://media.zjubiomedit.com/2019-08-26-032341.jpg" width=55%></p>
<p>把以上过程整理一下，就可以得到用于采样概率分布 <span class="math inline">\(p(x)\)</span> 的算法：</p>
<blockquote>
<p><strong>Algorithm</strong> MCMC 采样算法</p>
<ol type="1">
<li>初始化马氏链初始状态 <span class="math inline">\(X_0 = x_0\)</span></li>
<li>对 <span class="math inline">\(t=0,1,2,\cdots\)</span>，循环以下过程进行采样：
<ul>
<li>第 <span class="math inline">\(t\)</span> 个时刻马氏链状态未 <span class="math inline">\(X_t = x_t\)</span>，采样 <span class="math inline">\(y \sim q\left(x | x_{t}\right)\)</span></li>
<li>从均匀分布采样 <span class="math inline">\(u \sim U n i f o r m[0,1]\)</span></li>
<li>如果 <span class="math inline">\(u&lt;\alpha\left(x_{t}, y\right)=p(y) q\left(x_{t} | y\right)\)</span>，则接受转移 <span class="math inline">\(x_{t} \rightarrow y\)</span>，即 <span class="math inline">\(X_{t+1} = y\)</span></li>
<li>否则不接受转移，即 <span class="math inline">\(X_{t+1} = x_t\)</span></li>
</ul></li>
</ol>
</blockquote>
<ul>
<li>上述过程对连续分布以及高维空间均成立</li>
</ul>
<p>上述算法存在一个小问题：马氏链在转移过程中的接受率 <span class="math inline">\(\alpha(i,j)\)</span> 可能偏小，导致马氏链容易原地踏步，使得收敛到平稳分布的速度太慢。</p>
<p>注意到对于细致平稳条件来说，等式两边同时扩大并不会打破原有的等式，因此我们可以将 <span class="math inline">\(\alpha(i,j)\)</span> 和 <span class="math inline">\(\alpha(i,j)\)</span> 同比例放大，使得两数中最大的一个放大到 1： <span class="math display">\[
\alpha(i, j)=\min \left\{\frac{p(j) q(j, i)}{p(i) q(i, j)}, 1\right\}
\]</span> 于是，我们得到了下面的 Metropolis-Hastings 算法：</p>
<blockquote>
<p><strong>Algorithm</strong> Metropolis-Hastings 采样算法</p>
<ol type="1">
<li>初始化马氏链初始状态 <span class="math inline">\(X_0 = x_0\)</span></li>
<li>对 <span class="math inline">\(t=0,1,2,\cdots\)</span>，循环以下过程进行采样：
<ul>
<li>第 <span class="math inline">\(t\)</span> 个时刻马氏链状态为 <span class="math inline">\(X_t = x_t\)</span>，采样 <span class="math inline">\(y \sim q\left(x | x_{t}\right)\)</span></li>
<li>从均匀分布采样 <span class="math inline">\(u \sim U n i f o r m[0,1]\)</span></li>
<li>如果 <span class="math inline">\(u&lt;\alpha\left(x_{t}, y\right)=\min \left\{\frac{p(y) q\left(x_{t} | y\right)}{p\left(x_{t}\right) p\left(y | x_{t}\right)}, 1\right\}\)</span>，则接受转移 <span class="math inline">\(x_{t} \rightarrow y\)</span>，即 <span class="math inline">\(X_{t+1} = y\)</span></li>
<li>否则不接受转移，即 <span class="math inline">\(X_{t+1} = x_t\)</span></li>
</ul></li>
</ol>
</blockquote>
<h2 id="gibbs-sampling">Gibbs Sampling</h2>
<p>对于高维的情形，由于接受率的存在（通常 <span class="math inline">\(\alpha &lt; 1\)</span>），上述算法的效率并不是很高，我们希望找到一个转移矩阵 <span class="math inline">\(Q\)</span> 使得接受率 <span class="math inline">\(\alpha=1\)</span>。</p>
<p>对于二维的情形，假设有一个概率分布 <span class="math inline">\(p(x,y)\)</span>，考察 <span class="math inline">\(x\)</span> 坐标相同的两个点 <span class="math inline">\(A\left(x_{1}, y_{1}\right), B\left(x_{1}, y_{2}\right)\)</span>，可以发现： <span class="math display">\[
\begin{array}{l}{p\left(x_{1}, y_{1}\right) p\left(y_{2} | x_{1}\right)=p\left(x_{1}\right) p\left(y_{1} | x_{1}\right) p\left(y_{2} | x_{1}\right)} \\ {p\left(x_{1}, y_{2}\right) p\left(y_{1} | x_{1}\right)=p\left(x_{1}\right) p\left(y_{2} | x_{1}\right) p\left(y_{1} | x_{1}\right)}\end{array}
\]</span> 所以有： <span class="math display">\[
p\left(x_{1}, y_{1}\right) p\left(y_{2} | x_{1}\right)=p\left(x_{1}, y_{2}\right) p\left(y_{1} | x_{1}\right)
\]</span> 即： <span class="math display">\[
p(A) p\left(y_{2} | x_{1}\right)=p(B) p\left(y_{1} | x_{1}\right)
\]</span> 基于以上等式，我们发现在 <span class="math inline">\(x=x_1\)</span> 这条平行于 <span class="math inline">\(y\)</span> 轴的直线上，如果使用条件分布 <span class="math inline">\(p\left(y | x_{1}\right)\)</span> 作为任意两个点之间的转移概率，那么任意两个点之间的转移满足细致平稳条件。该结论对于平行于 <span class="math inline">\(x\)</span> 轴的直线也成立： <span class="math display">\[
p(A) p\left(x_{2} | y_{1}\right)=p(C) p\left(x_{1} | y_{1}\right)
\]</span> <img src="http://media.zjubiomedit.com/2019-08-30-075754.jpg" width=30%></p>
<p>因此，我们可以构造如下平面上任意两点之间的转移概率矩阵 <span class="math inline">\(Q\)</span> <span class="math display">\[
\begin{array}{ll}{Q(A \rightarrow B)=p\left(y_{B} | x_{1}\right)} &amp; {\text{if} \quad x_{A}=x_{B}=x_{1}} \\ {Q(A \rightarrow C)=p\left(x_{C} | y_{1}\right)} &amp; {\text{if}\quad y_{A}=y_{C}=y_{1}} \\ {Q(A \rightarrow D)=0} &amp; {\text {others}}\end{array}
\]</span> 基于上述转移矩阵 <span class="math inline">\(Q\)</span>，我们可以证明对于平面上任意两点 <span class="math inline">\(X,Y\)</span>，满足细致平稳条件： <span class="math display">\[
p(X) Q(X \rightarrow Y)=p(Y) Q(Y \rightarrow X)
\]</span> 于是这个二维空间上的马氏链将收敛到平稳分布 <span class="math inline">\(p(x,y)\)</span>，而这个算法就称为 <strong>Gibbs Sampling</strong> 算法：</p>
<blockquote>
<p><strong>Algorithm</strong> 二维 Gibbs Sampling 算法</p>
<ol type="1">
<li>随机初始化 <span class="math inline">\(X_0 = x_0, Y_0 = y_0\)</span></li>
<li>对 <span class="math inline">\(t=0,1,2,\cdots\)</span>，循环采样：
<ul>
<li><span class="math inline">\(y_{t+1} \sim p\left(y | x_{t}\right)\)</span></li>
<li><span class="math inline">\(y_{t+1} \sim p\left(y | x_{t}\right)\)</span></li>
</ul></li>
</ol>
</blockquote>
<p>以上采样过程中，马氏链的转义只是轮换地沿着坐标轴 <span class="math inline">\(x\)</span> 轴和 <span class="math inline">\(y\)</span> 轴作转移，最终收敛后得到的样本就是 <span class="math inline">\(p(x,y)\)</span> 的样本。可以采用坐标轴轮换，也可以随机选择坐标轴转移。</p>
<p>上述过程可以推广到高维，即沿着单根坐标轴转移，以其他所有坐标轴为条件：</p>
<blockquote>
<p><strong>Algorithm</strong> n 维 Gibbs Sampling 算法</p>
<ol type="1">
<li>随机初始化 <span class="math inline">\(\left\{x_{i} : i=1, \cdots, n\right\}\)</span></li>
<li>对 <span class="math inline">\(t=0,1,2,\cdots\)</span>，循环采样：
<ul>
<li><span class="math inline">\(x_{1}^{(t+1)} \sim p\left(x_{1} | x_{2}^{(t)}, x_{3}^{(t)}, \cdots, x_{n}^{(t)}\right)\)</span></li>
<li><span class="math inline">\(x_{2}^{(t+1)} \sim p\left(x_{2} | x_{1}^{(t+1)}, x_{3}^{(t)}, \cdots, x_{n}^{(t)}\right)\)</span></li>
<li><span class="math inline">\(\cdots\)</span></li>
<li><span class="math inline">\(x_{j}^{(t+1)} \sim p\left(x_{j} | x_{1}^{(t+1)}, \cdots, x_{j-1}^{(t+1)}, x_{j+1}^{(t)}, \cdots, x_{n}^{(t)}\right)\)</span></li>
<li><span class="math inline">\(\cdots\)</span></li>
<li><span class="math inline">\(x_{n}^{(t+1)} \sim p\left(x_{n} | x_{1}^{(t+1)}, x_{2}^{t}, \cdots, x_{n-1}^{(t+1)}\right)\)</span></li>
</ul></li>
</ol>
</blockquote>
<h1 id="文本建模">文本建模</h1>
<p>在日常生活中存在着大量的文本，如果每一个文本存储为一篇文档，那每篇文档从人的观察来说就是<strong>有序的词的序列</strong> <span class="math inline">\(d=\left(w_{1}, w_{2}, \cdots, w_{n}\right)\)</span>。</p>
<p><img src="http://media.zjubiomedit.com/2019-09-02-022004.jpg" width=35%></p>
<p>统计文本建模的目的就是追问这些观察到的语料库中的词序列是如何生成的。其核心的两个问题是：</p>
<ol type="1">
<li>模型中有哪些参数？</li>
<li>如何基于模型来产生词序列？</li>
</ol>
<p>下面我们将介绍在 LDA 之前出现的两种文本建模的方法：Unigram Model 和 PLSA Model。</p>
<h2 id="unigram-model">Unigram Model</h2>
<p>假设我们的词典中一共有 <span class="math inline">\(V\)</span> 个词，最简单的一种文本建模方式就是 Unigram Model，其方法如下：</p>
<blockquote>
<p><strong>Algorithm</strong> Unigram Model</p>
<ol type="1">
<li>上帝只有一个骰子，这个骰子有 <span class="math inline">\(V\)</span> 个面，每个面对应一个词，各个面的概率不一
<ul>
<li>即参数为词典中每个词语出现的概率 <span class="math inline">\(\vec{p}=\left(p_{1}, p_{2}, \cdots, p_{V}\right)\)</span></li>
<li>该模型并不关心词语的顺序信息</li>
</ul></li>
<li>每抛一次骰子，抛出的面就对应地产生一个词，假设总词频为 <span class="math inline">\(N\)</span>，则生成语料的方法为独立地抛 <span class="math inline">\(N\)</span> 次骰子产生这 <span class="math inline">\(N\)</span> 个词</li>
</ol>
</blockquote>
<p>基于上述方法，假定每个词 <span class="math inline">\(v_i\)</span> 的发生次数为 <span class="math inline">\(n_i\)</span>，则 <span class="math inline">\(\vec{n}=\left(n_{1}, n_{2}, \cdots, n_{V}\right)\)</span> 恰好为一个多项分布： <span class="math display">\[
p(\vec{n})=\operatorname{Mult}(\vec{n} ;\vec{p}, N)=\left(\begin{array}{c}{N} \\ {\vec{n}}\end{array}\right) \prod_{k=1}^{V} p_{k}^{n_{k}}
\]</span> 则语料的产生概率为： <span class="math display">\[
p(\mathcal{W})=p\left(\vec{w}_{1}\right) p\left(\vec{w}_{2}\right) \cdots p(\vec{w}_{m})=\prod_{k=1}^{V} p_{k}^{n_{k}}
\]</span> 参数的最大似然估计值为： <span class="math display">\[
\hat{p}_{i}=\frac{n_{i}}{N}
\]</span> 上述方法假定参数 <span class="math inline">\(\vec{p}\)</span> 是确定的，而从贝叶斯学派的角度看，一切参数都是随机变量。因此上述模型中的参数应该满足某个概率分布 <span class="math inline">\(p(\vec{p})\)</span>，该分布称为参数 <span class="math inline">\(\vec{p}\)</span> 的先验分布。修改后的算法流程如下：</p>
<blockquote>
<p><strong>Algorithm</strong> Bayesian Unigram Model</p>
<ol type="1">
<li>上帝有一个装有无穷多个骰子的坛子，里面有各式各样的骰子，每个骰子有 <span class="math inline">\(V\)</span> 个面</li>
<li>上帝从坛子里面抽了一个骰子出来，然后用这个骰子不断地抛，产生了语料中的所有的词</li>
</ol>
</blockquote>
<p>在上述假设下，语料的产生概率计算如下： <span class="math display">\[
p(\mathcal{W})=\int p(\mathcal{W} | \vec{p}) p(\vec{p}) d \vec{p}
\]</span> 那么如何选择先验分布 <span class="math inline">\(p(\vec{p})\)</span> 呢？注意到词语的发生次数满足多项分布，因此基于之前的结论，我们选择多项分布的共轭分布，即 Dirichlet 分布： <span class="math display">\[
\operatorname{Dir}(\vec{p} | \vec{\alpha})=\frac{1}{\Delta(\vec{\alpha})} \prod_{k=1}^{V} p_{k}^{\alpha_{k}-1} \quad \vec{\alpha}=\left(\alpha_{1}, \cdots, \alpha_{V}\right)
\]</span> 其中： <span class="math display">\[
\Delta(\vec{\alpha})=\int \prod_{k=1}^{V} p_{k}^{\alpha_{k}-1} d \vec{p}
\]</span> 可以通过下图理解 Dirichlet 先验下的 Unigram Model：</p>
<p><img src="http://media.zjubiomedit.com/2019-09-02-024609.jpg" width=40%></p>
<p>基于之前的结论，我们有如下等式： <span class="math display">\[
\operatorname{Dir}(\vec{p} | \vec{\alpha})+\operatorname{Mult}(\vec{n})=\operatorname{Dir}(\vec{p} | \vec{\alpha}+\vec{n})
\]</span> 因此后验分布为： <span class="math display">\[
p(\vec{p} | \mathcal{W}, \vec{\alpha})=\operatorname{Dir}(\vec{p} | \vec{\alpha}+\vec{n})=\frac{1}{\Delta(\vec{n}+\vec{\alpha})} \prod_{k=1}^{V} p_{k}^{n_{k}+\alpha_{k}-1} d \vec{p}
\]</span> 在贝叶斯框架下，我们取参数在后验分布下的平均值作为参数的估计值，基于之前的结论，我们有： <span class="math display">\[
E(\vec{p})=\left(\frac{n_{1}+\alpha_{1}}{\sum_{i=1}^{V}\left(n_{i}+\alpha_{i}\right)}, \frac{n_{2}+\alpha_{2}}{\sum_{i=1}^{V}\left(n_{i}+\alpha_{i}\right)}, \cdots, \frac{n_{V}+\alpha_{V}}{\sum_{i=1}^{V}\left(n_{i}+\alpha_{i}\right)}\right)
\]</span> 进一步，我们可以计算语料的产生概率（基于先验分布和后验分布下的估计值计算）为： <span class="math display">\[
\begin{aligned} p(\mathcal{W} | \vec{\alpha}) &amp;=\int p(\mathcal{W} | \vec{p}) p(\vec{p} | \vec{\alpha}) d \vec{p} \\ &amp;=\int \prod_{k=1}^{V} p_{k}^{n_{k}} \operatorname{Dir}(\vec{p} | \vec{\alpha}) d \vec{p} \\ &amp;=\int \prod_{k=1}^{V} p_{k}^{n_{k}} \frac{1}{\Delta(\vec{\alpha})} \prod_{k=1}^{V} p_{k}^{\alpha_{k}-1} d \vec{p} \\ &amp;=\frac{1}{\Delta(\vec{\alpha})} \int \prod_{k=1}^{V} p_{k}^{n_{k}+\alpha_{k}-1} d \vec{p} \\ &amp;=\frac{\Delta(\vec{n}+\vec{\alpha})}{\Delta(\vec{\alpha})} \end{aligned}
\]</span></p>
<h2 id="plsa-model">PLSA Model</h2>
<p>PLSA（Probabilistic Latent Semantic Analysis） Model 相比于 Unigram Model，引入了主题（Topic）的概念。PLSA Model 认为一篇文档可以由多个主题混合而成，而每个主题都是词汇上的概率分布，文档中的每个词都是由一个固定的主题生成的。</p>
<p>基于上述概念，PLSA Model 的流程如下：</p>
<blockquote>
<p><strong>Algorithm</strong> PLSA Model</p>
<ol type="1">
<li>上帝有两种类型的骰子
<ul>
<li>一类是 doc-topic 骰子，每个 doc-topic 骰子有 K 个面，每个面是一个 topic 的编号</li>
<li>一类是 topic-word 骰子，每个 topic-word 骰子有 <span class="math inline">\(V\)</span> 个面，每个面对应一个词</li>
</ul></li>
<li>上帝共有 <span class="math inline">\(K\)</span> 个 topic-word 骰子，每个骰子都有一个编号，编号从 <span class="math inline">\(1\)</span> 到 <span class="math inline">\(K\)</span></li>
<li>生成每篇文档之前，上帝都先为这篇文章制造一个特定的 doc-topic 骰子，然后重复如下过程生成文档中的词：
<ul>
<li>投掷这个 doc-topic 骰子，得到一个 topic 编号 <span class="math inline">\(z\)</span></li>
<li>选择 <span class="math inline">\(K\)</span> 个 topic-word 骰子中编号为 <span class="math inline">\(z\)</span> 的那个，投掷这个骰子，于是得到一个词</li>
</ul></li>
</ol>
</blockquote>
<p>上述过程可以用下图表示：</p>
<p><img src="http://media.zjubiomedit.com/2019-09-04-075810.jpg" width=40%></p>
<p>该模型的数学表达如下：</p>
<ul>
<li><p>游戏中的 <span class="math inline">\(K\)</span> 个 topic-word 骰子记为 <span class="math inline">\(\vec{\varphi}_{1}, \cdots, \vec{\varphi}_{K}\)</span></p></li>
<li><p>对于包含 <span class="math inline">\(M\)</span> 篇文档的语料 <span class="math inline">\(C=\left(d_1,d_2,\cdots，d_M\right)\)</span> 中的每篇文档 <span class="math inline">\(d_m\)</span>，都会有一个特定的 doc-topic 骰子 <span class="math inline">\(\vec{\theta}_m\)</span></p>
<ul>
<li>所有对应的骰子记为 <span class="math inline">\(\vec{\theta}_1,\cdots, \vec{\theta}_M\)</span></li>
</ul></li>
<li><p>在 PLSA Model 中，第 <span class="math inline">\(m\)</span> 篇文档 <span class="math inline">\(d_m\)</span> 中的每个词的生成概率为： <span class="math display">\[
p\left(w | d_{m}\right)=\sum_{z=1}^{K} p(w | z) p\left(z | d_{m}\right)=\sum_{z=1}^{K} \varphi_{z w} \theta_{m z}
\]</span></p></li>
<li><p>因此整篇文档的生成概率为： <span class="math display">\[
p\left(\vec{w} | d_{m}\right)=\prod_{i=1}^{n} \sum_{z=1}^{K} p\left(w_{i} | z\right) p\left(z | d_{m}\right)=\prod_{i=1}^{n} \sum_{z=1}^{K} \varphi_{z w_{i}} \theta_{d z}
\]</span></p></li>
</ul>
<p>求解上述模型中的参数可以使用 EM 算法（因为包含隐含变量 <span class="math inline">\(z\)</span>），这里不作展开。</p>
<h1 id="lda-文本建模">LDA 文本建模</h1>
<p>对于 PLSA Model，doc-topic 骰子 <span class="math inline">\(\vec{\theta}_m\)</span> 和 topic-word 骰子 <span class="math inline">\(\vec{\varphi}_m\)</span> 都是模型中的参数，但并没有看作随机变量。如果使用贝叶斯学派的观点，将上述参数看作随机变量，为其添加先验分布，即可将 PLSA 对应的游戏过程改造为一个贝叶斯的游戏过程。</p>
<p>由于 <span class="math inline">\(\vec{\theta}_m\)</span> 和 <span class="math inline">\(\vec{\varphi}_m\)</span> 都对应到多项分布，所以先验分布的一个好的选择就是 Dirichlet 分布，于是我们就得到了 LDA（Latent Dirichlet Allocation）模型。</p>
<blockquote>
<p><strong>Algorithm</strong> LDA Model</p>
<ol type="1">
<li>上帝有两大坛子的骰子，第一个坛子装的是 doc-topic 骰子，第二个坛子装的是 topic-word 骰子</li>
<li>上帝随机地从第二个坛子中独立地抽取了 <span class="math inline">\(K\)</span> 个 topic-word 骰子，编号为 <span class="math inline">\(1\)</span> 到 <span class="math inline">\(K\)</span></li>
<li>每次生成一篇新的文档前，上帝先从第一个坛子中随机抽取一个 doc-topic 骰子，然后重复如下过程生成文档中的词：
<ul>
<li>投掷这个 doc-topic 骰子，得到一个 topic 编号 <span class="math inline">\(z\)</span></li>
<li>选择 <span class="math inline">\(K\)</span> 个 topic-word 骰子中编号为 <span class="math inline">\(z\)</span> 的那个，投掷这个骰子，于是得到一个词</li>
</ul></li>
</ol>
</blockquote>
<p>上述过程可以用下图表示：</p>
<p><img src="http://media.zjubiomedit.com/2019-09-05-013456.jpg" width=45%></p>
<p>假设语料库中有 <span class="math inline">\(M\)</span> 篇文档，所有的 word 和对应的 topic 如下表示： <span class="math display">\[
\begin{aligned} \vec{\mathbf{w}} &amp;=\left(\vec{w}_{1}, \cdots, \vec{w}_{M}\right) \\ \vec{\mathbf{z}} &amp;=\left(\vec{z}_{1}, \cdots, \vec{z}_{M}\right) \end{aligned}
\]</span></p>
<ul>
<li><span class="math inline">\(\vec{w}_m\)</span> 表示第 <span class="math inline">\(m\)</span> 篇文档中的词</li>
<li><span class="math inline">\(\vec{z}_m\)</span> 表示这些词对应的 topic 编号</li>
</ul>
<p><img src="http://media.zjubiomedit.com/2019-09-05-013847.jpg" width=35%></p>
<h2 id="物理过程分解">物理过程分解</h2>
<p>我们可以将 LDA 模型的游戏过程用如下概率图模型表示：</p>
<p><img src="http://media.zjubiomedit.com/2019-09-05-021751.jpg" width=30%></p>
<p>该概率图可以分解为两个主要的物理过程：</p>
<ol type="1">
<li><span class="math inline">\(\vec{\alpha} \rightarrow \vec{\theta}_{m} \rightarrow z_{m, n}\)</span>，这个过程表示在生成第 <span class="math inline">\(m\)</span> 篇文档的时候，先从第一个坛子抽了一个 doc-topic 骰子 <span class="math inline">\(\vec{\theta}_m\)</span>，然后投掷这个骰子生成了文档中第 <span class="math inline">\(n\)</span> 个词的 topic 编号 <span class="math inline">\(z_{m,n}\)</span></li>
<li><span class="math inline">\(\vec{\beta} \rightarrow \vec{\varphi}_{k} \rightarrow w_{m, n} | k=z_{m, n}\)</span>，这个过程表示在上帝手头的 <span class="math inline">\(K\)</span> 个 topic-word 骰子 <span class="math inline">\(\vec{\varphi}_k\)</span> 中，挑选编号为 <span class="math inline">\(k=z_{m,n}\)</span> 的那个骰子进行投掷，然后生成 word <span class="math inline">\(w_{m,n}\)</span></li>
</ol>
<p>在 LDA 模型中，有一些物理过程是相互独立可交换的，<span class="math inline">\(M\)</span> 篇文档会对应 <span class="math inline">\(M\)</span> 个独立的 Dirichlet-Multinomial 共轭结构，<span class="math inline">\(K\)</span> 个 topic 会对应 <span class="math inline">\(K\)</span> 个独立的 Dirichlet-Multinomial 共轭结构，因此 LDA 模型可以被分解为 <span class="math inline">\(M+K\)</span> 个 Dirichlet-Multinomial 共轭结构，下面我们来具体阐述这两个结构。</p>
<h3 id="m-个-dirichlet-multinomial-共轭结构">M 个 Dirichlet-Multinomial 共轭结构</h3>
<p>对于第一个物理过程，<span class="math inline">\(\vec{\alpha} \rightarrow \vec{\theta}_{m} \rightarrow \vec{z}_{m}\)</span> 表示生成第 <span class="math inline">\(m\)</span> 篇文档中所有词对应的 topics，显然 <span class="math inline">\(\vec{\alpha} \rightarrow \vec{\theta}_{m}\)</span> 对应于 Dirichlet 分布，<span class="math inline">\(\vec{\theta}_{m} \rightarrow \vec{z}_{m}\)</span> 对应于 Multinomial 分布，所以该过程整体是一个 Dirichlet-Multinomial 结构： <span class="math display">\[
\vec{\alpha} \underbrace{\longrightarrow}_{\text {Dirichlet }} \vec{\theta}_{m} \underbrace{\longrightarrow}_{\text {Multinomial }} \vec{z}_{m}
\]</span> 利用 Dirichlet-Multinomial 共轭结构，我们得到参数 <span class="math inline">\(\vec{\theta}_m\)</span> 的后验分布恰好是： <span class="math display">\[
\operatorname{Dir}\left(\vec{\theta}_{m} | \vec{n}_{m}+\vec{\alpha}\right)
\]</span> 基于之前 Bayesian Unigram Model 中的计算结果，我们可以得到： <span class="math display">\[
p\left(\vec{z}_{m} | \vec{\alpha}\right)=\frac{\Delta\left(\vec{n}_{m}+\vec{\alpha}\right)}{\Delta(\vec{\alpha})}
\]</span></p>
<ul>
<li>其中 <span class="math inline">\(\vec{n}_{m}=\left(n_{m}^{(1)}, \cdots, n_{m}^{(K)}\right)\)</span> 表示第 <span class="math inline">\(m\)</span> 篇文档中不同 topic 产生的词的个数（总数为该篇文档的词语数）</li>
</ul>
<p>由于语料中 <span class="math inline">\(M\)</span> 篇文档的 topics 生成过程相互独立，所以我们得到了 <span class="math inline">\(M\)</span> 个相互独立的 Dirichlet-Multinomial 共轭结构，从而可以得到整个语料中 topics 的生成概率： <span class="math display">\[
\begin{aligned} p(\vec{\mathbf{z}} | \vec{\alpha}) &amp;=\prod_{m=1}^{M} p\left(\vec{z}_{m} | \vec{\alpha}\right) \\ &amp;=\prod_{m=1}^{M} \frac{\Delta\left(\vec{n}_{m}+\vec{\alpha}\right)}{\Delta(\vec{\alpha})} \end{aligned}
\]</span></p>
<h3 id="k-个-dirichlet-multinomial-共轭结构">K 个 Dirichlet-Multinomial 共轭结构</h3>
<p>为了得到这 <span class="math inline">\(K\)</span> 个共轭结构，我们需要对游戏的顺序作如下调整：</p>
<blockquote>
<p><strong>Algorithm</strong> LDA Model 2</p>
<ol type="1">
<li>上帝有两大坛子的骰子，第一个坛子装的是 doc-topic 骰子，第二个坛子装的是 topic-word 骰子</li>
<li>上帝随机地从第二个坛子中独立地抽取了 <span class="math inline">\(K\)</span> 个 topic-word 骰子，编号为 <span class="math inline">\(1\)</span> 到 <span class="math inline">\(K\)</span></li>
<li>每次生成一篇新的文档前，上帝先从第一个坛子中随机抽取一个 doc-topic 骰子，然后重复投掷这个 doc-topic 骰子，为每个词生成一个 topic 编号 <span class="math inline">\(z\)</span>
<ul>
<li>重复如上过程处理每篇文档，生成语料中每个词的 topic 编号，但是词尚未生成</li>
</ul></li>
<li>从头到尾，对语料中的每篇文档中的每个 topic 编号 <span class="math inline">\(z\)</span>，选择 <span class="math inline">\(K\)</span> 个 topic-word 骰子中编号为 <span class="math inline">\(z\)</span> 的那个，投掷这个骰子，于是生成对应的 word</li>
</ol>
</blockquote>
<p>以上游戏是先生成了语料中所有词的 topic，然后对每个词在给定 topic 的条件下生成 word。由于使用的是词袋模型，词语之间的顺序信息并没有被考虑，所以上述过程与之前的游戏是等价的。于是我们把具有相同 topic 的词归类： <span class="math display">\[
\begin{aligned} \overrightarrow{\mathbf{w}}^{\prime} &amp;=\left(\vec{w}_{(1)}, \cdots, \vec{w}_{(K)}\right) \\ \overrightarrow{\mathbf{z}}^{\prime} &amp;=\left(\vec{z}_{(1)}, \cdots, \vec{z}_{(K)}\right) \end{aligned}
\]</span></p>
<ul>
<li><span class="math inline">\(\vec{w}_{(k)}\)</span> 表示这些词都是由第 <span class="math inline">\(k\)</span> 个 topic 生成的</li>
<li><span class="math inline">\(\vec{z}_{(k)}\)</span> 对应于这些词的 topic 编号，所以 <span class="math inline">\(\vec{z}_{(k)}\)</span> 中的分量都是 <span class="math inline">\(k\)</span></li>
</ul>
<p>对于第二个物理过程，考虑如下过程 <span class="math inline">\(\vec{\beta} \rightarrow \vec{\varphi}_{k} \rightarrow \vec{w}_{(k)}\)</span>，此时 <span class="math inline">\(\vec{\beta} \rightarrow \vec{\varphi}_{k}\)</span> 对应于 Dirichlet 分布，<span class="math inline">\(\vec{\varphi}_{k} \rightarrow \vec{w}_{(k)}\)</span> 对应于 Multinomial 分布，所以整体也是一个 Dirichlet-Multinomial 共轭结构： <span class="math display">\[
\vec{\beta} \underbrace{\longrightarrow}_{\text {Dirichlet }} \vec{\varphi}_{k} \underbrace{\longrightarrow}_{\text {Multinomial }} \vec{w}_{(k)}
\]</span> 类似地，我们得到参数 <span class="math inline">\(\vec{\varphi}_{k}\)</span> 的后验分布恰好是： <span class="math display">\[
\operatorname{Dir}\left(\vec{\varphi}_{k} | \vec{n}_{k}+\vec{\beta}\right)
\]</span> 同理，我们可以得到： <span class="math display">\[
p\left(\vec{w}_{(k)} | \vec{\beta}\right)=\frac{\Delta\left(\vec{n}_{k}+\vec{\beta}\right)}{\Delta(\vec{\beta})}
\]</span></p>
<ul>
<li>其中 <span class="math inline">\(\vec{n}_{k}=\left(n_{k}^{(1)}, \cdots, n_{k}^{(V)}\right)\)</span> 表示第 <span class="math inline">\(k\)</span> 个 topic 产生的词中不同词语的个数（总数为第 <span class="math inline">\(k\)</span> 个 topic 产生的词语总数，取决于上一步）</li>
</ul>
<p>由于语料中 <span class="math inline">\(K\)</span> 个 topics 生成 words 的过程相互独立，所以我们得到 <span class="math inline">\(K\)</span> 个相互独立的 Dirichlet-Multinomial 共轭结构，从而得到整个语料库中词生成概率： <span class="math display">\[
\begin{aligned} p(\overrightarrow{\mathbf{w}} | \overrightarrow{\mathbf{z}}, \vec{\beta}) &amp;=p\left(\overrightarrow{\mathbf{w}}^{\prime} | \overrightarrow{\mathbf{z}}^{\prime}, \vec{\beta}\right) \\ &amp;=\prod_{k=1}^{K} p\left(\vec{w}_{(k)} | \vec{z}_{(k)}, \vec{\beta}\right) \\ &amp;=\prod_{k=1}^{K} \frac{\Delta\left(\vec{n}_{k}+\vec{\beta}\right)}{\Delta(\vec{\beta})} \end{aligned}
\]</span> 将上述两个过程结合起来，得到： <span class="math display">\[
\begin{aligned} p(\vec{\mathbf{w}}, \vec{\mathbf{z}} | \vec{\alpha}, \vec{\beta}) &amp;=p(\vec{\mathbf{w}} | \vec{\mathbf{z}}, \vec{\beta}) p(\vec{\mathbf{z}} | \vec{\alpha}) \\ &amp;=\prod_{k=1}^{K} \frac{\Delta(\vec{n}_{k}+\vec{\beta})}{\Delta(\vec{\beta})} \prod_{m=1}^{M} \frac{\Delta(\vec{n}_{m}+\vec{\alpha})}{\Delta(\vec{\alpha})} \end{aligned}
\]</span></p>
<h2 id="gibbs-sampling-1">Gibbs Sampling</h2>
<p>得到了联合分布 <span class="math inline">\(p(\vec{\mathbf{w}}, \vec{\mathbf{z}})\)</span> 之后，我们可以使用 Gibbs Sampling 对这个分布进行采样。由于 <span class="math inline">\(\vec{w}\)</span> 是观测到的已知数据，只有 <span class="math inline">\(\vec{z}\)</span> 是隐含变量，因此真正需要采样的是分布 <span class="math inline">\(p(\vec{z} |\vec{w} )\)</span>。</p>
<p>为了更好地理解采样过程，本节我们将基于 Dirichlet-Multinomial 共轭来推导 Gibbs Sampling 公式。</p>
<p>语料库 <span class="math inline">\(\vec{z}\)</span> 中的第 <span class="math inline">\(i\)</span> 个词对应的 topic 记为 <span class="math inline">\(z_i\)</span>，其中 <span class="math inline">\(i=(m,n)\)</span> 是一个二维下标，对应于第 <span class="math inline">\(m\)</span> 篇文档的第 <span class="math inline">\(n\)</span> 个词，我们用 <span class="math inline">\(\neg i\)</span> 表示去除下标为 <span class="math inline">\(i\)</span> 的词。</p>
<p>按照 Gibbs Sampling 算法（高维）的要求，我们要求得任意一个坐标轴 <span class="math inline">\(i\)</span> 对应的条件分布 <span class="math inline">\(p\left(z_{i}=k | \vec{\mathbf{z}}_{\neg i}, \vec{\mathbf{w}}\right)\)</span>，假设已经观测到的词 <span class="math inline">\(w_i=t\)</span>，则由贝叶斯法则，我们容易得到： <span class="math display">\[
p\left(z_{i}=k | \vec{\mathbf{z}}_{\neg i}, \vec{\mathbf{w}}\right) \propto p\left(z_{i}=k, w_{i}=t | \vec{\mathbf{z}}_{\neg i}, \vec{\mathbf{w}}_{\neg i}\right)
\]</span> 由于 <span class="math inline">\(z_{i}=k, w_{i}=t\)</span> 只涉及第 <span class="math inline">\(m\)</span> 篇文档和 第 <span class="math inline">\(k\)</span> 个 topic，所以上式的条件概率计算中，实际上也只会涉及到如下两个 Dirichlet-Multinomial 共轭结构：</p>
<ol type="1">
<li><span class="math inline">\(\vec{\alpha} \rightarrow \vec{\theta}_{m} \rightarrow \vec{z}_{m}\)</span></li>
<li><span class="math inline">\(\vec{\beta} \rightarrow \vec{\varphi}_{k} \rightarrow \vec{w}_{(k)}\)</span></li>
</ol>
<p>另一方面，在语料中去掉第 <span class="math inline">\(i\)</span> 个词对应的 <span class="math inline">\((z_i,w_i)\)</span>，并不改变我们之前讨论的 <span class="math inline">\(M+K\)</span> 个共轭结构，只是某些地方的计数会变少。所以其后验分布为： <span class="math display">\[
\begin{array}{l}{p\left(\vec{\theta}_{m} | \overrightarrow{\mathbf{z}}_{\neg i}, \overrightarrow{\mathbf{w}}_{\neg i}\right)=\operatorname{Dir}\left(\vec{\theta}_{m} | \vec{n}_{m,\neg i}+\vec{\alpha}\right)} \\ {p\left(\vec{\varphi}_{k} | \overrightarrow{\mathbf{z}}_{\neg i}, \overrightarrow{\mathbf{w}}_{\neg i}\right)=\operatorname{Dir}\left(\vec{\varphi}_{k} | \vec{n}_{k,\neg i}+\vec{\beta}\right)}\end{array}
\]</span> 基于上面的想法，通过一系列推导，可以得出如下的 Gibbs Sampling 公式： <span class="math display">\[
\begin{aligned} p\left(z_{i}=k | \vec{\mathbf{z}}_{\neg i}, \vec{\mathbf{w}}\right) &amp; \propto p\left(z_{i}=k, w_{i}=t | \vec{\mathbf{z}}_{\neg i}, \vec{\mathbf{w}}_{\neg i}\right) \\ &amp;=\hat{\theta}_{m k} \cdot \hat{\varphi}_{k t} \end{aligned}
\]</span> 最终得到的结果就是对应的两个 Dirichlet 后验分布在贝叶斯框架下的参数估计，基于之前的公式，我们有： <span class="math display">\[
\begin{aligned} \hat{\theta}_{m k} &amp;=\frac{n_{m,\neg{i}}^{(k)}+\alpha_{k}}{\sum_{k=1}^{K}\left(n_{m,\neg{i}}^{(t)}+\alpha_{k}\right)} \\ \hat{\varphi}_{k t} &amp;=\frac{n_{k,\neg{i}}^{(t)}+\beta_{t}}{\sum_{t=1}^{V}\left(n_{k,\neg{i}}^{(t)}+\beta_{t}\right)} \end{aligned}
\]</span> 于是，我们得到了如下 LDA 模型的 Gibbs Sampling 公式： <span class="math display">\[
p\left(z_{i}=k | \vec{\mathbf{z}}_{\neg i}, \vec{\mathbf{w}}\right)  \propto \frac{n_{m,\neg{i}}^{(k)}+\alpha_{k}}{\sum_{k=1}^{K}\left(n_{m,\neg{i}}^{(t)}+\alpha_{k}\right)} \cdot \frac{n_{k,\neg{i}}^{(t)}+\beta_{t}}{\sum_{t=1}^{V}\left(n_{k,\neg{i}}^{(t)}+\beta_{t}\right)}
\]</span> 由于 topic 有 <span class="math inline">\(K\)</span> 个，所以 Gibbs Sampling 公式的物理意义其实就是在这 <span class="math inline">\(K\)</span> 条路径中进行采样。</p>
<p><img src="http://media.zjubiomedit.com/2019-09-05-084257.jpg" width=45%></p>
<h2 id="training-and-inference">Training and Inference</h2>
<p>对于 LDA 模型，我们的目标有两个：</p>
<ol type="1">
<li>估计模型中的参数 <span class="math inline">\(\vec{\varphi}_{1}, \cdots, \vec{\varphi}_{K}\)</span> 和 <span class="math inline">\(\vec{\theta}_{1}, \cdots, \vec{\theta}_{M}\)</span></li>
<li>对于新来的一篇文档，能够计算这篇文档的 topic 分布 <span class="math inline">\(\vec{\theta}_{new}\)</span></li>
</ol>
<p>训练 LDA 模型的过程是通过 Gibbs Sampling 获取语料中的 <span class="math inline">\((z,w)\)</span> 的样本，模型中的所有参数都可以基于最终采样得到的样本进行估计。具体流程如下：</p>
<blockquote>
<p><strong>Algorithm</strong> LDA Training</p>
<ol type="1">
<li>随机初始化：对语料中每篇文档中的每个词 <span class="math inline">\(w\)</span>，随机地赋一个 topic 编号 <span class="math inline">\(z\)</span></li>
<li>重新扫描语料库，对每个词 <span class="math inline">\(w\)</span> ，基于 Gibbs Sampling 公式重新采样其 topic，在语料库中进行更新</li>
<li>重复以上语料库的重新采样过程直到 Gibbs Sampling 收敛</li>
<li>统计语料库的 topic-word 共现频率矩阵，该矩阵就是 LDA 的模型</li>
</ol>
</blockquote>
<p>根据上述频率矩阵我们可以算出模型参数 <span class="math inline">\(\vec{\varphi}_{1}, \cdots, \vec{\varphi}_{K}\)</span> 与 <span class="math inline">\(\vec{\theta}_{1}, \cdots, \vec{\theta}_{M}\)</span>，而在实际应用中，参数 <span class="math inline">\(\vec{\theta}\)</span> 和训练语料中的每篇文档相关，对于理解新的文档并无用处，所以工程上最终存储 LDA 模型的时候一般没有必要保留。</p>
<p>通常，在 LDA 模型训练的过程中，我们是取 Gibbs Sampling 收敛之后的 <span class="math inline">\(n\)</span> 个迭代的结果进行平均来做参数估计，这样模型质量更高。</p>
<p>有了 LDA 模型，对于新来的文档，我们只要认为 Gibbs Sampling 公式中的 <span class="math inline">\(\hat{\varphi}_{k t}\)</span> 部分是稳定不变的，由训练语料得到的模型提供，采样过程中只需要估计该文档的 topic 分布 <span class="math inline">\(\vec{\theta}_{new}\)</span> 即可。</p>
<blockquote>
<p><strong>Algorithm</strong> LDA Inference</p>
<ol type="1">
<li>随机初始化：对当前文档中的每个词 <span class="math inline">\(w\)</span>，随机地赋一个 topic 编号 <span class="math inline">\(z\)</span></li>
<li>重新扫描当前文档，按照 Gibbs Sampling 公式，对每个词 <span class="math inline">\(w\)</span>，重新采样它的 topic</li>
<li>重复以上过程直到 Gibbs Sampling 收敛</li>
<li>统计文档中的 topic 分布，该分布就是 <span class="math inline">\(\vec{\theta}_{new}\)</span></li>
</ol>
</blockquote>

    </div>

    
    
    

    <footer class="post-footer">




<div class="license">
  <div class="license-title">LDA 原理第一部分：LDA 数学八卦</div>
  <div class="license-link">
    <a href="https://xxwywzy.github.io/2019/08/26/lda-1/">https://xxwywzy.github.io/2019/08/26/lda-1/</a>
  </div>
  <div class="license-meta">
    <div class="license-meta-item">
      <div class="license-meta-title">本文作者</div>
      <div class="license-meta-text">
          Zheyu Wang
      </div>
    </div>
      <div class="license-meta-item">
        <div class="license-meta-title">发布于</div>
        <div class="license-meta-text">
          2019-08-26
        </div>
      </div>
      <div class="license-meta-item">
        <div class="license-meta-title">更新于</div>
        <div class="license-meta-text">
          2023-08-05
        </div>
      </div>
    <div class="license-meta-item">
      <div class="license-meta-title">许可协议</div>
      <div class="license-meta-text">
          <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" rel="noopener" target="_blank">CC BY-NC-SA 4.0</a>
      </div>
    </div>
  </div>
  <div class="license-statement">
      转载或引用本文时，请遵守上述许可协议，注明出处、不得用于商业用途！
  </div>
</div>
          <div class="post-tags">
              <a href="/tags/LDA/" rel="tag"># LDA</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2019/07/18/protege/" rel="prev" title="本体入门之二：OWL 本体构建指南">
                  <i class="fa fa-angle-left"></i> 本体入门之二：OWL 本体构建指南
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2019/09/06/music-coursera-2/" rel="next" title="音乐理论基础-第二周学习笔记">
                  音乐理论基础-第二周学习笔记 <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments" id="lv-container" data-id="city" data-uid="MTAyMC81ODgyNi8zNTI4OA=="></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2023</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Zheyu Wang</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
      <span>站点总字数：</span>
    <span title="站点总字数">332k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span>站点阅读时长 &asymp;</span>
    <span title="站点阅读时长">18:26</span>
  </span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  






  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<script src="/js/third-party/comments/livere.js"></script>



  <style>
    #taboola-livere { display: none;}
  </style>



<script type="text/javascript">
var linkLists = document.querySelectorAll(".link-list");

linkLists.forEach(function(linkList) {
  var listPath = linkList.getAttribute('json-src');
  var iconPath = linkList.getAttribute('icon-src');
  
  var xhr = new XMLHttpRequest();
  xhr.open('GET', listPath, true);
  xhr.onreadystatechange = function() {
    if (xhr.readyState === 4 && xhr.status === 200) {
      var data = JSON.parse(xhr.responseText);
      
      var li = "";
      linkList.innerHTML = '';

      for (var infoIndex = 0; infoIndex < data.length; infoIndex++) {
        var info = data[infoIndex];
        var labelWarn = info['warn'] ? '<span class="label warn">' + info['warn'] + '</span>' : '';
        var labelInfo = info['info'] ? '<span class="label info">' + info['info'] + '</span>' : '';

        li += '<div class="link-list-container">';
        li += '<img class="link-list-image" src="' + iconPath + info['logo'] + '">';
        li += '<p>' + info['title'] + labelInfo + labelWarn + '</p>';
        li += '<p>' + info['intro'] + '</p>';
        li += '<a href="' + info['url'] + '" rel="noopener" target="_blank" data-pjax-state=""></a>';
        li += '</div>';
      }
      
      linkList.innerHTML = li;
    }
  };
  xhr.send();
});
</script>


<script type="text/javascript">
var cultureList = document.querySelectorAll(".culture-list");
if (cultureList.length !== 0) {
  var j = -1;
  for (var i = 0; i < cultureList.length; i++) {
    const listPath = cultureList[i].getAttribute('json-src');
    const coverPath = cultureList[i].getAttribute('cover-src');
    
    var xhr = new XMLHttpRequest();
    xhr.open('GET', listPath, true);
    xhr.onreadystatechange = function () {
      if (xhr.readyState === 4 && xhr.status === 200) {
        j++;
        var data = JSON.parse(xhr.responseText);
        var li = "";
        
        cultureList[j].innerHTML = '';

        for (var infoIndex = 0; infoIndex < data.length; infoIndex++) {
          var info = data[infoIndex];
          
          var title = info['title'];
          if (info['link']) {
            title = '<a href="' + info['link'] + '">' + info['title'] + '</a>';
          }

          var author = info['author'] ? '<span class="author">' + info['author'] + '</span>' : '';

          var intro = info['intro'] ? info['intro'] : '';

          var star = '';
          if (info['score'] == null) {
            star = '';
          } else {
            var colorStar = '';
            var greyStar = '';
            var int = Math.floor(info['score']); //整数部分
            var fract = 0;
            if (info['score'] % 1 !== 0) {
              fract = 1;
            }
            for (var m = 0; m < int; m++) {
              colorStar += '★';
            }
            if (fract !== 0) {
              colorStar += '☆';
            }
            for (var m = 0; m < (5 - fract - int); m++) {
              greyStar += '☆';
            }
            if (info['score'] !== 5) {
              star = '<span class="star-score">' + colorStar + '<span class="grey-star">' + greyStar + '</span></span>';
            } else {
              star = '<span class="star-score">' + colorStar + '</span>';
            }
          }

          li += '<div class="media">';
          li += '<div class="media-cover" style="background-image:url(' + coverPath + info['cover'] + ')"></div>';
          li += '<div class="media-meta">';
          li += '<div class="media-meta-item title">' + title + '</div>';
          li += '<div class="media-meta-item">' + author + star + '</div>';
          li += '<div class="media-meta-item intro">' + intro + '</div>';
          li += '</div></div>';
        }
        
        cultureList[j].innerHTML = li;
      }
    };
    xhr.send();
  }
}
</script>




<script src="/resources/minigrid.min.js"></script>
<script type="text/javascript">
var album = document.querySelector(".album");
if (album) {
  // 相册列表 JSON 数据
  var imgDataPath = album.getAttribute('json-src');
  // 照片存储路径
  var imgPath = album.getAttribute('photo-src');
  // 最多显示数量
  var imgMaxNum = 50;
  // 获取窗口大小以决定图片宽度
  var windowWidth = window.innerWidth || document.documentElement.clientWidth || document.body.clientWidth;
  var imageWidth;

  if (windowWidth < 768) {
    imageWidth = 145; // 移动端图片宽度
  } else {
    imageWidth = 235;
  }

  // 腾讯云自定义样式 (数据万象外网流量需要付费)
  //var imgStyle = '!' + imageWidth + 'x';
  //var imgStyle = '!300x';

  // 生成相册
  var linkDataPath = imgDataPath;
  var photo = {
    page: 1,
    offset: imgMaxNum,
    init: function () {
      var that = this;
      var xhr = new XMLHttpRequest();
      xhr.open("GET", linkDataPath, true);
      xhr.onreadystatechange = function () {
        if (xhr.readyState === 4 && xhr.status === 200) {
          var data = JSON.parse(xhr.responseText);
          that.render(that.page, data);
        }
      };
      xhr.send();
    },
    render: function (page, data) {
      var begin = (page - 1) * this.offset;
      var end = page * this.offset;
      if (begin >= data.length) return;
      var imgNameWithPattern, imgName, imageSize, imageX, imageY, li = "";
      for (var i = begin; i < end && i < data.length; i++) {
        imgNameWithPattern = data[i].split(' ')[1];
        imgName = imgNameWithPattern.split('.')[0];
        imageSize = data[i].split(' ')[0];
        imageX = imageSize.split('.')[0];
        imageY = imageSize.split('.')[1];
        li += '<div class="card" style="width:' + imageWidth + 'px" >';
        li += '<div class="album-photo" style="height:'+ imageWidth * imageY / imageX + 'px">';
        li += '<a class="fancybox fancybox.image" href="' + imgPath + imgNameWithPattern + '" itemscope="" itemtype="http://schema.org/ImageObject" itemprop="url" data-fancybox="group" rel="group" data-caption="' + imgName + '" title="' +  imgName + '">';
        li += '<img data-src="' + imgPath + imgNameWithPattern + '" src="' + imgPath + imgNameWithPattern + '" alt="' +  imgName + '" data-loaded="true">';
        li += '</a>';
        li += '</div>';
        li += '</div>';
      }
      album.insertAdjacentHTML('beforeend', li);
      this.minigrid();
    },
    minigrid: function () {
      var grid = new Minigrid({
        container: '.album',
        item: '.card',
        gutter: 12
      });
      grid.mount();
      window.addEventListener('resize', function () {
        grid.mount();
      });
    }
  };
  photo.init();
}
</script>
</body>
</html>
