<!DOCTYPE html>












  


<html class="theme-next gemini use-motion" lang="zh-CN">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2"/>
<meta name="theme-color" content="#222"/>


























<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2"/>

<link rel="stylesheet" href="/css/main.css?v=7.0.0"/>


  <link rel="apple-touch-icon" sizes="180x180" href="/images/favicon.png?v=7.0.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon.png?v=7.0.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.png?v=7.0.0">


  <link rel="mask-icon" href="/images/favicon.png?v=7.0.0" color="#222">







<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '7.0.0',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":true,"onmobile":false},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="本篇博客为 CS229 学习笔记第一部分，主题是：线性回归。">
<meta name="keywords" content="CS229,线性回归">
<meta property="og:type" content="article">
<meta property="og:title" content="CS229 学习笔记之一：线性回归">
<meta property="og:url" content="https://xxwywzy.github.io/2018/03/14/CS229-1/index.html">
<meta property="og:site_name" content="xxwywzy&#39;s Blog">
<meta property="og:description" content="本篇博客为 CS229 学习笔记第一部分，主题是：线性回归。">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="http://media.zjubiomedit.com/2019-02-05-060331.png">
<meta property="og:image" content="http://media.zjubiomedit.com/2019-02-05-062839.png">
<meta property="og:updated_time" content="2019-03-24T09:48:34.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="CS229 学习笔记之一：线性回归">
<meta name="twitter:description" content="本篇博客为 CS229 学习笔记第一部分，主题是：线性回归。">
<meta name="twitter:image" content="http://media.zjubiomedit.com/2019-02-05-060331.png">






  <link rel="canonical" href="https://xxwywzy.github.io/2018/03/14/CS229-1/"/>



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>CS229 学习笔记之一：线性回归 | xxwywzy's Blog</title>
  






  <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?a02b5462e7522b1ed191c4cea6b1d6e6";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>







  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">xxwywzy's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
      
        <p class="site-subtitle">Long may the sunshine</p>
      
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">

    
    
    
      
    

    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br/>首页</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-about">

    
    
    
      
    

    

    <a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i> <br/>关于</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">

    
    
    
      
    

    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br/>标签</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">

    
    
    
      
    

    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br/>分类</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
    
      
    

    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br/>归档</a>

  </li>

      
      
    </ul>
  

  

  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
            

          
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://xxwywzy.github.io/2018/03/14/CS229-1/"/>

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Zheyu Wang"/>
      <meta itemprop="description" content="相信过程"/>
      <meta itemprop="image" content="/images/avatar.png"/>
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="xxwywzy's Blog"/>
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">CS229 学习笔记之一：线性回归

              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2018-03-14 22:06:32" itemprop="dateCreated datePublished" datetime="2018-03-14T22:06:32+08:00">2018-03-14</time>
            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/人工智能/" itemprop="url" rel="index"><span itemprop="name">人工智能</span></a></span>

                
                
                  ，
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/人工智能/机器学习/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a></span>

                
                
              
            </span>
          

          
            
            
              
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <a href="/2018/03/14/CS229-1/#comments" itemprop="discussionUrl">
                  <span class="post-meta-item-text">评论数：</span> <span class="post-comments-count valine-comment-count" data-xid="/2018/03/14/CS229-1/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
            <span id="/2018/03/14/CS229-1/" class="leancloud_visitors" data-flag-title="CS229 学习笔记之一：线性回归">
              <span class="post-meta-divider">|</span>
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              
                <span class="post-meta-item-text">阅读次数：</span>
              
                <span class="leancloud-visitors-count"></span>
            </span>
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <div class="note info">
            本篇博客为 CS229 学习笔记第一部分，主题是：线性回归。 
          </div>
<a id="more"></a>
<h1 id="算法简介">算法简介</h1>
<ul>
<li>线性回归是⼀种监督学习算法，即给定⼀个训练集，去学习⼀个假设函数，⽤来尽量精确地预测每个样本对应的输出
<ul>
<li>线性回归属于回归算法，其输出变量连续</li>
<li>另⼀类监督学习算法是分类算法，其输出变量离散</li>
</ul></li>
<li><p>线性回归的假设函数为： <span class="math display">\[
h_\theta(x) = \sum_{i=0}^n\theta_ix_i = \theta^Tx
\]</span></p></li>
<li><p>线性回归的代价函数为： <span class="math display">\[
J(\theta) = \frac 1 2 \sum_{i=1}^m \left(h_\theta(x^{(i)})-y^{(i)}\right)^2
\]</span></p></li>
<li><p>线性回归的⽬的：通过训练集找出使代价函数最小的一组参数 <span class="math inline">\(\theta\)</span>（又称最小二乘法）</p></li>
</ul>
<h1 id="求解方法">求解方法</h1>
<ul>
<li>对于线性回归代价函数的求解，有两种可选⽅法：<strong>梯度下降</strong>与<strong>正规⽅程</strong></li>
</ul>
<h2 id="梯度下降">梯度下降</h2>
<ul>
<li>梯度下降是⼀种求解最优化问题的迭代⽅法，具体步骤为：
<ol type="1">
<li>随机选取初始的 <span class="math inline">\(\theta\)</span></li>
<li>不断地以梯度的方向修正 <span class="math inline">\(\theta\)</span></li>
<li>最终使 <span class="math inline">\(J(\theta)\)</span> 收敛至局部最优（在最小二乘中，局部最优即全局最优） <span class="math display">\[
\theta_j := \theta_j-\alpha\frac{\partial}{\partial \theta_j}J(\theta)\tag{1}
\]</span></li>
</ol></li>
<li><span class="math inline">\(\alpha\)</span> 称为学习速率，太小会导致收敛缓慢，太大会导致错过最优点，需要谨慎选择</li>
<li><p>对公式进一步推导（假设只有一个样本点），得到： <span class="math display">\[
\begin{align*}
\frac{\partial}{\partial \theta_j}J(\theta) &amp;=\frac{\partial}{\partial \theta_j}\frac 1 2(h_\theta(x)-y)^2 \\&amp;=2\cdot\frac1 2(h_\theta(x)-y)\cdot\frac{\partial}{\partial \theta_j}(h_\theta(x)-y) \\&amp;=(h_\theta(x)-y)\cdot\frac{\partial}{\partial \theta_j}\left(\sum_{i=0}^n\theta_ix_i-y\right) \\&amp;=(h_\theta(x)-y)x_j
\end{align*}
\]</span></p></li>
<li><p>将上述结果代⼊ (1) 式得到： <span class="math display">\[
\theta_j := \theta_j+\alpha\left(y^{(i)}-h_\theta(x^{(i)})\right)x^{(i)}_j\tag{2}
\]</span></p>
<ul>
<li>上述的结果通过数学变换将减号变成了加号，方便之后与逻辑回归的结果作比较</li>
</ul></li>
</ul>
<h3 id="分类">分类</h3>
<ul>
<li>梯度下降主要可以分为两类：<strong>批量梯度下降</strong>和<strong>随机梯度下降</strong></li>
<li>批量梯度下降：每次计算梯度都需要遍历所有的样本点，当样本量很大时， 计算速度会十分缓慢</li>
<li>随机梯度下降：每次只考虑⼀个样本点，而不是所有样本点，计算速度会提高，但是收敛过程会⽐较曲折，可能⽆法精确收敛⾄最优值
<ul>
<li>随机梯度下降的优化：小批量梯度下降，利用矩阵并⾏运算，⼀次处理小批量的样本点，有时可以比随机梯度下降速度更快</li>
</ul></li>
</ul>
<h3 id="梯度方向的选择">梯度方向的选择</h3>
<ul>
<li>选择梯度⽅向的原因是它是使代价函数减⼩（下降）最⼤的⽅向
<ul>
<li>我们可以利⽤柯⻄不等式对这⼀结论进⾏证明</li>
</ul></li>
<li><p>当 <span class="math inline">\(\theta\)</span> 改变一个很小的量时，利用泰勒公式，忽略一阶导数之后的项，得： <span class="math display">\[
\Delta J \thickapprox \frac{\partial J}{\partial \theta_0} \Delta \theta_0+ \frac{\partial J}{\partial \theta_1} \Delta \theta_1+\cdots+ \frac{\partial J}{\partial \theta_n} \Delta \theta_n \tag{3}
\]</span></p></li>
<li><p>定义如下变量： <span class="math display">\[
\begin{split}
\Delta\theta &amp;\equiv (\Delta\theta_0,\Delta\theta_1,\ldots,\Delta\theta_n)^T \\
\nabla J &amp;\equiv (\frac{\partial J}{\partial \theta_0},\frac{\partial J}{\partial \theta_1},\ldots,\frac{\partial J}{\partial \theta_n})^T
\end{split}
\]</span></p></li>
<li><p>将其代回 <span class="math inline">\((3)\)</span> 式，得： <span class="math display">\[
\Delta J \thickapprox \nabla J\cdot\Delta \theta
\]</span></p></li>
<li><p>根据柯西不等式，有（等号当且仅当 <span class="math inline">\(\Delta \theta\)</span> 与 <span class="math inline">\(\nabla J\)</span> 线性相关时成立）： <span class="math display">\[
\lvert\Delta J \rvert\,\thickapprox \,\lvert\nabla J\cdot\Delta \theta \rvert\,\le\,\|\nabla J\|\cdot\|\Delta \theta \|
\]</span></p></li>
<li><p>因此，要使 <span class="math inline">\(\Delta J\)</span> 最小，即 <span class="math inline">\(\mid\Delta J \mid\)</span> 最大且 <span class="math inline">\(\Delta J&lt;0\)</span>，而当且仅当 <span class="math inline">\(\Delta \theta = -\alpha \nabla J \quad(\alpha&gt;0)\)</span> 时满足条件，即沿着梯度方向调整 <span class="math inline">\(\theta\)</span></p></li>
</ul>
<h2 id="正规方程">正规方程</h2>
<ul>
<li>我们可以通过正规方程直接求出 <span class="math inline">\(\theta\)</span> 的解析解。推导过程如下：</li>
</ul>
<h3 id="矩阵导数">矩阵导数</h3>
<ul>
<li><p>对一个将 <span class="math inline">\(m\times n\)</span> 的矩阵映射至实数的函数，定义其导数为： <span class="math display">\[
\nabla_A f(A) = \left[
\begin{matrix}
 \frac{\partial f}{\partial A_{11}}     &amp; \cdots &amp; \frac{\partial f}{\partial A_{1n}}      \\
 \vdots  &amp; \ddots &amp; \vdots \\
  \frac{\partial f}{\partial A_{m1}}     &amp; \cdots &amp; \frac{\partial f}{\partial A_{mn}}     \\
\end{matrix}
\right]
\]</span></p></li>
<li><p>对一个 <span class="math inline">\(n\times n\)</span> 方阵，它的迹定义为对角线元素之和： <span class="math display">\[
\text{tr}\,A = \sum_{i=1}^nA_{ii}
\]</span></p></li>
<li><p>易证明迹操作具有如下性质（各矩阵为⽅阵）： <span class="math display">\[
\mbox{tr}\,AB = \mbox{tr}\,BA,\\
\mbox{tr}\,ABC = \mbox{tr}\,CAB = \mbox{tr}\,BCA,\\
\mbox{tr}\,ABCD = \mbox{tr}\,DABC = \mbox{tr}\,CDAB = \mbox{tr}\,BCDA
\]</span></p></li>
<li><p>同样易证明如下性质（<span class="math inline">\(a\)</span> 为实数）： <span class="math display">\[
\mbox{tr}\,A = \mbox{tr}\,A^T\\
\mbox{tr}\,(A+B) = \mbox{tr}\,A+ \mbox{tr}\,B\\
\mbox{tr}\,aA = a\,\mbox{tr}\,A
\]</span></p></li>
<li><p>基于以上定义，可以证明一些关于矩阵导数的性质（等式 <span class="math inline">\((7)\)</span> 只针对非奇异矩阵）： <span class="math display">\[
\begin{eqnarray*}
\nabla_A\mbox{tr}\,AB &amp;=&amp; B^T \tag{4} \\
\nabla_{A^T}f(A) &amp;=&amp; (\nabla_{A}f(A))^T \tag{5} \\
\nabla_A\mbox{tr}\,ABA^TC &amp;=&amp; CAB+C^TAB^T \tag{6} \\
\nabla_A\lvert A \rvert &amp;=&amp; \lvert A \rvert(A^{-1})^T \tag{7}
\end{eqnarray*}
\]</span></p></li>
</ul>
<h3 id="最小二乘重现">最小二乘重现</h3>
<ul>
<li><p>对于训练集，可以写成如下的形式： <span class="math display">\[
X = \left[
\begin{matrix}
 —(x^{(1)})^T—  \\ 
  —(x^{(2)})^T—   \\
 \vdots   \\
  —(x^{(m)})^T— \\
\end{matrix}
\right], \; \vec y=\left[
\begin{matrix}
 y^{(1)}  \\ 
  y^{(2)}   \\
 \vdots   \\
  y^{(m)} \\
\end{matrix}
\right]
\]</span></p></li>
<li><p>因为 <span class="math inline">\(h_{\theta}(x^{(i)}) = (x^{(i)})^T\theta\)</span>，我们可以得出： <span class="math display">\[
\begin{align*}
 X\theta-\vec y&amp;=\left[
\begin{matrix}
 (x^{(1)})^T\theta  \\ 
  (x^{(2)})^T\theta   \\
 \vdots   \\
  (x^{(m)})^T\theta \\
\end{matrix}
\right]-\left[
\begin{matrix}
 y^{(1)}  \\ 
  y^{(2)}   \\
 \vdots   \\
  y^{(m)} \\
\end{matrix}
\right] \\
&amp;=\left[
\begin{matrix}
 h_{\theta}(x^{(1)})-y^{(1)}  \\ 
  h_{\theta}(x^{(2)})-y^{(2)}   \\
 \vdots   \\
  h_{\theta}(x^{(m)})-y^{(m)} \\
\end{matrix}
\right] 
 \end{align*}
\]</span></p></li>
<li><p>此外，对于一个向量z，我们有 <span class="math inline">\(z^Tz = \sum_iz_i^2\)</span>，因此综上可以得出： <span class="math display">\[
\begin{align*}
 \frac 1 2(X\theta-\vec y)^T(X\theta-\vec y) &amp;= \frac 1 2 \sum_{i=1}^m \left(h_\theta(x^{(i)})-y^{(i)}\right)^2\\
 &amp;= J(\theta)
  \end{align*}
\]</span></p></li>
<li>所以为了使 <span class="math inline">\(J(\theta)\)</span> 最小，即只需找出其导数为 0 时 <span class="math inline">\(\theta\)</span> 的值。下面给出详细的求解过程：
<ul>
<li><p>首先，将 <span class="math inline">\((5)\)</span> 式与 <span class="math inline">\((6)\)</span> 式结合，得： <span class="math display">\[
\begin{eqnarray*}
\nabla_{A^T}f(A) &amp;=&amp; (\nabla_{A}f(A))^T \tag{5} \\
\nabla_A\mbox{tr}\,ABA^TC &amp;=&amp; CAB+C^TAB^T \tag{6} \\
\nabla_{A^T}\mbox{tr}\,ABA^TC &amp;=&amp;  (CAB+C^TAB^T)^T\\
&amp;=&amp; (CAB)^T+(C^TAB^T)^T \\
&amp;=&amp; B^TA^TC^T+BA^TC\tag{8}
\end{eqnarray*}
\]</span></p>
<ul>
<li>其中最后两步的推导基于矩阵转置的下列性质： <span class="math display">\[
(A+B)^T=A^T+B^T \\
(AB)^T=B^TA^T
\]</span></li>
</ul></li>
<li><p>基于以上所述，有： <span class="math display">\[
\begin{align*}
\nabla_\theta J(\theta) &amp;= \nabla  \frac 1 2(X\theta-\vec y)^T(X\theta-\vec y) \\
&amp;= \frac 1 2 \nabla_\theta(\theta^TX^T-\vec y^T)(X\theta-\vec y)\\
&amp;= \frac 1 2 \nabla_\theta\underbrace{(\theta^TX^TX\theta-\theta^TX^T\vec y-\vec y^TX\theta+\vec y^T\vec y)}_{a = \text {tr}\,a,\,a\in R}\\
&amp;= \frac 1 2 \nabla_\theta\underbrace{\text{tr}\,(\theta^TX^TX\theta-\theta^TX^T\vec y-\vec y^TX\theta+\vec y^T\vec y)}_{ \text {tr}\,(A+B)=\text {tr}\,A+\text {tr}\,B}\\
&amp;= \frac 1 2 \nabla_\theta\underbrace{(\text {tr}\,\theta^TX^TX\theta-\text {tr}\,\theta^TX^T\vec y-\text {tr}\,\vec y^TX\theta+\text {tr}\,\vec y^T\vec y)}_{\text{tr}\,A = \text{tr}\,A^T}\\
&amp;= \frac 1 2 \nabla_\theta(\text {tr}\,\theta^TX^TX\theta-2\text {tr}\,\vec y^TX\theta+\underbrace{\text {tr}\,\vec y^T\vec y}_{\text{don&#39;t depend on }\theta})\\
&amp;= \frac 1 2 \nabla_\theta(\text {tr}\,\theta^TX^TX\theta-2\text {tr}\,\vec y^TX\theta)\\
&amp;= \frac 1 2 (\underbrace{\nabla_\theta\text {tr}\,\theta^TX^TX\theta}_{\text{use }(8),\,A^T=\theta,B=B^T=X^TX,C=I}-2\underbrace{\nabla_\theta\text {tr}\,\vec y^TX\theta}_{\text{tr}\,ABC = \text{tr}\,CAB,\text{then use }(4)})\\
&amp;= \frac 1 2(X^TX\theta+X^TX\theta-2X^T\vec y)\\
&amp;=X^TX\theta-X^T\vec y
\end{align*}
\]</span></p></li>
</ul></li>
<li><p>因此，正规方程如下： <span class="math display">\[
X^TX\theta=X^T\vec y\\
\theta=(X^TX)^{-1}X^T\vec y
\]</span></p>
<ul>
<li>不可逆问题可以通过伪逆计算或正则化处理解决</li>
</ul></li>
</ul>
<h1 id="概率解释">概率解释</h1>
<ul>
<li>在线性回归中，为什么要选择最小二乘函数作为代价函数？我们可以用概率模型来对其进行解释</li>
</ul>
<h2 id="概率模型">概率模型</h2>
<ul>
<li><p>假设真实值与输入之间满足如下等式： <span class="math display">\[
y^{(i)} = \theta^Tx^{(i)}+\epsilon^{(i)}
\]</span></p>
<ul>
<li>其中 <span class="math inline">\(\epsilon^{(i)}\)</span> 是误差项，表示没有被建模的因素或是随机噪声</li>
</ul></li>
<li><p>进一步假设误差项是独立同分布的，那么根据中心极限定理，大量相互独立的随机变量符合以 0 为中心的正态分布（可以理解为大量独立随机变量的大部分误差会相互抵消），即 <span class="math inline">\(\epsilon^{(i)}\sim {\cal N} (0,\sigma^2)\)</span>，那么有： <span class="math display">\[
p(\epsilon^{(i)}) = \frac{1}{\sqrt{2\pi}\sigma}\text{exp}\left(-\frac{(\epsilon^{(i)})^2}{2\sigma^2}\right)
\]</span></p></li>
<li><p>而误差的概率和预测出真实值的概率是一样的，因此： <span class="math display">\[
p(y^{(i)} \,\rvert \,x^{(i)};\theta ) = \frac{1}{\sqrt{2\pi}\sigma}\text{exp}\left(-\frac{(y^{(i)} - \theta^Tx^{(i)})^2}{2\sigma^2}\right)
\]</span></p>
<ul>
<li>注意，这里 <span class="math inline">\(p(y^{(i)} \,\rvert \,x^{(i)};\theta )\)</span> 不同于 <span class="math inline">\(p(y^{(i)} \,\rvert \,x^{(i)},\theta )\)</span>，这里指给定 <span class="math inline">\(x^{(i)}\)</span>，以 <span class="math inline">\(\theta\)</span> 为参数的 <span class="math inline">\(y^{(i)}\)</span> 的分布，因为对于训练集，<span class="math inline">\(\theta\)</span> 是客观存在的，只是当前还不确定（这是一种频率学派的观点，之后会细说）</li>
</ul></li>
<li><p>因此，我们得出真实值是以预测值为中心的一个正态分布： <span class="math display">\[
y^{(i)} \,\rvert \,x^{(i)};\theta\sim {\cal N }(\theta^Tx^{(i)},\sigma^2)
\]</span></p></li>
</ul>
<h2 id="似然函数">似然函数</h2>
<ul>
<li><p>给定训练集 <span class="math inline">\({X}\)</span> 和参数 <span class="math inline">\(\theta\)</span>，预测结果等于真实结果的概率，将其看作 <span class="math inline">\(\theta\)</span> 的函数，可以理解为 <span class="math inline">\(\theta\)</span> 为真实 <span class="math inline">\(\theta\)</span> 的可能性（似然性），即： <span class="math display">\[
L(\theta)=L(\theta;X,\vec y)=p(\vec y  \,\rvert\, X;\theta)
\]</span></p></li>
<li><p>因为每个样本是独立同分布的，所以有： <span class="math display">\[
\begin{align*}
L(\theta) &amp;= \prod_{i=1}^m p(y^{(i)}\,\lvert\,x^{(i)};\theta)\\
&amp;=\prod_{i=1}^m  \frac{1}{\sqrt{2\pi}\sigma}\text{exp}\left(-\frac{(y^{(i)} - \theta^Tx^{(i)})^2}{2\sigma^2}\right)
\end{align*}
\]</span></p></li>
<li>现在，我们可以通过<strong>最大似然法</strong>，即找出使 <span class="math inline">\(L(\theta)\)</span> 最大的那个 <span class="math inline">\(\theta\)</span>，作为对参数 <span class="math inline">\(\theta\)</span> 的最佳取值</li>
<li><p>实际应用中，为了简化计算，通常不直接求似然函数的最大值，而是采用<strong>对数似然</strong>函数： <span class="math display">\[
\begin{align*}
\ell(\theta) &amp;= \text{log}L(\theta)\\
&amp;= \text{log}\prod_{i=1}^m \frac{1}{\sqrt{2\pi}\sigma}\text{exp}\left(-\frac{(y^{(i)} - \theta^Tx^{(i)})^2}{2\sigma^2}\right)\\
&amp;= \sum_{i=1}^m\text{log}\frac{1}{\sqrt{2\pi}\sigma}\text{exp}\left(-\frac{(y^{(i)} - \theta^Tx^{(i)})^2}{2\sigma^2}\right)\\
&amp;=m\text{log}\frac{1}{\sqrt{2\pi}\sigma}-\frac{1}{\sigma^2} \cdot \frac 1 2 \sum_{i=1}^m(y^{(i)} - \theta^Tx^{(i)})^2
\end{align*}
\]</span></p></li>
<li><p>因此，最大化 <span class="math inline">\(l(\theta)\)</span> 就是最小化： <span class="math display">\[
\frac 1 2 \sum_{i=1}^m(y^{(i)} - \theta^Tx^{(i)})^2
\]</span></p>
<ul>
<li>这正是我们之前提出的<strong>最小二乘代价函数</strong></li>
<li>可以看出 <span class="math inline">\(\theta\)</span> 的选择并不依赖于 <span class="math inline">\(\sigma^2\)</span></li>
</ul></li>
<li><p>概率解释只是对最小二乘法的一种合理解释，其实还有其他的解释方法</p></li>
</ul>
<h1 id="局部加权线性回归">局部加权线性回归</h1>
<ul>
<li>本节将介绍⼀种特殊的线性回归算法</li>
</ul>
<h2 id="欠拟合与过拟合">欠拟合与过拟合</h2>
<ul>
<li><p>对于传统的线性回归，特征的选择极为重要，对于下面三幅图，我们称第一幅图的模型是<strong>欠拟合</strong>，第三幅图的模型则是<strong>过拟合</strong>（之后会详细介绍）</p>
<p><img src="http://media.zjubiomedit.com/2019-02-05-060331.png" width="75%"></p>
<ul>
<li>可以看出，找到一个全局的线性模型去拟合整个训练集，并不是一件简单的事情，往往会引起欠拟合或是过拟合的发生</li>
<li>对于这种情况之后会给出解决方案，而这里我们提出了另外一种思路，即<strong>局部线性加权回归</strong>，这种方案可以使特征的选择的重要性降低</li>
</ul></li>
</ul>
<h2 id="算法思路">算法思路</h2>
<ul>
<li>局部线性加权回归的思路是并不去拟合整个训练集来产生全局的模型，而是在每次预测时，只去拟合给定输入 <span class="math inline">\(x\)</span> 附近的一小段训练集，无论全局训练集是怎样的一条分布曲线，在局部小段数据上，都可以用线性去逼近。具体步骤如下：
<ol type="1">
<li>拟合 <span class="math inline">\(\theta\)</span> 来最小化 <span class="math inline">\(\sum_i\omega^{(i)}(y^{(i)} - \theta^Tx^{(i)})^2\)</span></li>
<li>输出 <span class="math inline">\(\theta^Tx\)</span></li>
</ol></li>
<li><p>这里 <span class="math inline">\(\omega^{(i)}\)</span> 是<strong>非负权重</strong>，一般取值如下： <span class="math display">\[
\omega^{(i)}=exp\left(-\frac{(x^{(i)}-x)^2}{2\tau^2}\right)
\]</span></p>
<ul>
<li>当 <span class="math inline">\(x\)</span> 为向量时表达式有所不同</li>
<li>可以看出，离给定输入越近的样本点权重越大，拟合程度越高</li>
</ul></li>
<li><span class="math inline">\(\omega^{(i)}\)</span> 的定义与高斯分布类似，但并没有关系，分布曲线同为钟型
<ul>
<li><span class="math inline">\(\tau\)</span> 称为<strong>带宽参数</strong>，用来控制钟型曲线的顶峰下降速度，即权重变化的快慢，需要根据具体情况作出调整</li>
</ul></li>
</ul>
<h2 id="参数学习与非参数学习">参数学习与非参数学习</h2>
<ul>
<li>局部加权线性回归本质上是一种<strong>非参数学习算法</strong>，而传统的线性回归是一种<strong>参数学习算法</strong></li>
<li>两者的区别在于：
<ul>
<li>参数学习算法有一组有限的、固定的参数，一旦完成拟合，只需要保存下参数值做预测，而不需要保存完整的训练集</li>
<li>非参数学习算法由于参数不固定，所以需要保存完整的训练集来进行预测，而不仅仅是保存参数</li>
</ul></li>
<li><strong>非参数学习</strong>导致的结果：为了表达假设 <span class="math inline">\(h\)</span> 而保存的数据将随着训练集的大小而线性增长</li>
</ul>
<h1 id="思维导图">思维导图</h1>
<p><img src="http://media.zjubiomedit.com/2019-02-05-062839.png" width="80%"></p>

      
    </div>

    

    
    
    

    

    
      
    
    

    
      <div>
        



  



<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>Zheyu Wang</li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    
    <a href="https://xxwywzy.github.io/2018/03/14/CS229-1/" title="CS229 学习笔记之一：线性回归">https://xxwywzy.github.io/2018/03/14/CS229-1/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！</li>
</ul>

      </div>
    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/CS229/" rel="tag"># CS229</a>
          
            <a href="/tags/线性回归/" rel="tag"># 线性回归</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/03/15/CS229-2/" rel="prev" title="CS229学习笔记之二：分类与逻辑回归">
                CS229学习笔记之二：分类与逻辑回归 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>


  </div>


          </div>
          

  
    <div class="comments" id="comments">
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/avatar.png"
                alt="Zheyu Wang"/>
            
              <p class="site-author-name" itemprop="name">Zheyu Wang</p>
              <p class="site-description motion-element" itemprop="description">相信过程</p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">52</span>
                    <span class="site-state-item-name">日志</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  <a href="/categories/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">16</span>
                    <span class="site-state-item-name">分类</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  <a href="/tags/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">22</span>
                    <span class="site-state-item-name">标签</span>
                  </a>
                </div>
              
            </nav>
          

          

          
            <div class="links-of-author motion-element">
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="https://github.com/xxwywzy" title="GitHub &rarr; https://github.com/xxwywzy" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
                </span>
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="https://twitter.com/xxwywzy" title="Twitter &rarr; https://twitter.com/xxwywzy" rel="noopener" target="_blank"><i class="fa fa-fw fa-twitter"></i>Twitter</a>
                </span>
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="http://weibo.com/xxwywzy" title="Weibo &rarr; http://weibo.com/xxwywzy" rel="noopener" target="_blank"><i class="fa fa-fw fa-weibo"></i>Weibo</a>
                </span>
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="https://instagram.com/xxwywzy" title="Instagram &rarr; https://instagram.com/xxwywzy" rel="noopener" target="_blank"><i class="fa fa-fw fa-instagram"></i>Instagram</a>
                </span>
              
            </div>
          

          

          
          

          
            
          
          

        </div>
      </div>

      
      <!--noindex-->
        <div class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
            
            
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#算法简介"><span class="nav-number">1.</span> <span class="nav-text">算法简介</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#求解方法"><span class="nav-number">2.</span> <span class="nav-text">求解方法</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#梯度下降"><span class="nav-number">2.1.</span> <span class="nav-text">梯度下降</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#分类"><span class="nav-number">2.1.1.</span> <span class="nav-text">分类</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#梯度方向的选择"><span class="nav-number">2.1.2.</span> <span class="nav-text">梯度方向的选择</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#正规方程"><span class="nav-number">2.2.</span> <span class="nav-text">正规方程</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#矩阵导数"><span class="nav-number">2.2.1.</span> <span class="nav-text">矩阵导数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#最小二乘重现"><span class="nav-number">2.2.2.</span> <span class="nav-text">最小二乘重现</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#概率解释"><span class="nav-number">3.</span> <span class="nav-text">概率解释</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#概率模型"><span class="nav-number">3.1.</span> <span class="nav-text">概率模型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#似然函数"><span class="nav-number">3.2.</span> <span class="nav-text">似然函数</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#局部加权线性回归"><span class="nav-number">4.</span> <span class="nav-text">局部加权线性回归</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#欠拟合与过拟合"><span class="nav-number">4.1.</span> <span class="nav-text">欠拟合与过拟合</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#算法思路"><span class="nav-number">4.2.</span> <span class="nav-text">算法思路</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#参数学习与非参数学习"><span class="nav-number">4.3.</span> <span class="nav-text">参数学习与非参数学习</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#思维导图"><span class="nav-number">5.</span> <span class="nav-text">思维导图</span></a></li></ol></div>
            

          </div>
        </div>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Zheyu Wang</span>

  

  
</div>


  <div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v3.5.0</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> v7.0.0</div>




        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
          <span id="scrollpercent"><span>0</span>%</span>
        
      </div>
    

    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  <script src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>


  


  <script src="/js/src/utils.js?v=7.0.0"></script>

  <script src="/js/src/motion.js?v=7.0.0"></script>



  
  


  <script src="/js/src/affix.js?v=7.0.0"></script>

  <script src="/js/src/schemes/pisces.js?v=7.0.0"></script>




  
  <script src="/js/src/scrollspy.js?v=7.0.0"></script>
<script src="/js/src/post-details.js?v=7.0.0"></script>



  


  <script src="/js/src/bootstrap.js?v=7.0.0"></script>



  
  

<script src="//cdn1.lncld.net/static/js/3.11.1/av-min.js"></script>



<script src="//unpkg.com/valine/dist/Valine.min.js"></script>

<script>
  var GUEST = ['nick', 'mail', 'link'];
  var guest = 'nick,mail,link';
  guest = guest.split(',').filter(function(item) {
    return GUEST.indexOf(item) > -1;
  });
  new Valine({
    el: '#comments',
    verify: false,
    notify: false,
    appId: 'FDq9lQI6SeKwqcOLjtAnvkN1-gzGzoHsz',
    appKey: 'IxP5URFEhxow4TfWyVNiowbH',
    placeholder: '请在这里评论=￣ω￣=',
    avatar: 'retro',
    meta: guest,
    pageSize: '10' || 10,
    visitor: false
  });
</script>




  


  





  
  
  <script>
    
    function addCount(Counter) {
      var $visitors = $('.leancloud_visitors');
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();

      Counter('get', '/classes/Counter', { where: JSON.stringify({ url }) })
        .done(function({ results }) {
          if (results.length > 0) {
            var counter = results[0];
            
              var $element = $(document.getElementById(url));
              $element.find('.leancloud-visitors-count').text(counter.time + 1);
            
            Counter('put', '/classes/Counter/' + counter.objectId, JSON.stringify({ time: { '__op': 'Increment', 'amount': 1 } }))
            
              .fail(function ({ responseJSON }) {
                console.log(`Failed to save Visitor num, with error message: ${responseJSON.error}`);
              })
          } else {
            
              Counter('post', '/classes/Counter', JSON.stringify({ title: title, url: url, time: 1 }))
                .done(function() {
                  var $element = $(document.getElementById(url));
                  $element.find('.leancloud-visitors-count').text(1);
                })
                .fail(function() {
                  console.log('Failed to create');
                });
            
          }
        })
        .fail(function ({ responseJSON }) {
          console.log(`LeanCloud Counter Error: ${responseJSON.code} ${responseJSON.error}`);
        });
    }
    

    $(function() {
      $.get('https://app-router.leancloud.cn/2/route?appId=' + 'FDq9lQI6SeKwqcOLjtAnvkN1-gzGzoHsz')
        .done(function({ api_server }) {
          var Counter = function(method, url, data) {
            return $.ajax({
              method: method,
              url: 'https://' + api_server + '/1.1' + url,
              headers: {
                'X-LC-Id': 'FDq9lQI6SeKwqcOLjtAnvkN1-gzGzoHsz',
                'X-LC-Key': 'IxP5URFEhxow4TfWyVNiowbH',
                'Content-Type': 'application/json',
              },
              data: data
            });
          };
          
            addCount(Counter);
          
        });
    });
  </script>



  

  

  

  
  

  
  

  
    
      <script type="text/x-mathjax-config">
  

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      
      equationNumbers: {
        autoNumber: "AMS"
      }
    }
  });
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
      for (i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
      }
  });
</script>
<script src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>

<style>
.MathJax_Display {
  overflow: auto hidden;
}
</style><!-- hexo-inject:begin --><!-- hexo-inject:end -->

    
  


  

  

  

  

  

  

  

  

</body>
</html>
